[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "coreR May 2025",
    "section": "",
    "text": "About the course\nThese are the instructional materials for the May 2025 coreR course!",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "coreR May 2025",
    "section": "NCEAS Expertise",
    "text": "NCEAS Expertise\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\n\nLearning Objectives\n\nEffectively manage data using tidy data practices\nImplement reproducible scientific workflows throughout all aspects of a project\nEstablish best practices and utilize tools like Git & GitHub to optimize your collaboration\nBetter communicate scientific analyses and results using Markdown, GitHub webpages, and R packages like ggplot2 and Shiny\nIncrease your familiarity and confidence with data science tools\n\n\n\nWeek’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "coreR May 2025",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "coreR May 2025",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Camila Vargas Poulsen & Angel Chen (2024), NCEAS coreR for Delta Science Program, June 2024, NCEAS Learning Hub. URL https://learning.nceas.ucsb.edu/2024-06-delta.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matt Jone, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "Reproducibility: a Lego Activity",
    "section": "",
    "text": "1.1 Reproducibility activity using LEGO®",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility: a Lego Activity</span>"
    ]
  },
  {
    "objectID": "session_01.html#reproducibility-activity-using-lego",
    "href": "session_01.html#reproducibility-activity-using-lego",
    "title": "Reproducibility: a Lego Activity",
    "section": "",
    "text": "Learning Objectives\n\nIllustrate elements of good reproducibility through the medium of LEGO®\nDiscuss what is needed and what is not needed for good reproducibility\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis activity is largely based on the LEGO® Metadata for Reproducibility game pack, which was developed by Mary Donaldson and Matt Mahon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility: a Lego Activity</span>"
    ]
  },
  {
    "objectID": "session_01.html#getting-started",
    "href": "session_01.html#getting-started",
    "title": "Reproducibility: a Lego Activity",
    "section": "1.2 Getting started",
    "text": "1.2 Getting started\n\n\n\n\n\n\nSetup\n\n\n\n\nGather into small groups\nGet LEGO® blocks and worksheets (instructions + metadata documentation)\nFollow directions on worksheets\n\n\n\nAt the end, we will discuss as a group.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility: a Lego Activity</span>"
    ]
  },
  {
    "objectID": "session_01.html#discussion",
    "href": "session_01.html#discussion",
    "title": "Reproducibility: a Lego Activity",
    "section": "1.3 Discussion",
    "text": "1.3 Discussion\n\n\nDiscussion Questions\n\n\nDid you find this a simple way to document your process?\nWas there anything you found difficult to capture?\nDid those replicating the builds find it straightforward to follow?\nDid you encounter any ambiguity in the instructions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility: a Lego Activity</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "Collaborating with Git and GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "Collaborating with Git and GitHub",
    "section": "",
    "text": "Apply the principles, features, and collaboration tools of Git and GitHub to effectively collaborate with colleagues on code\nAnalyze and evaluate common causes of conflicts that arise when collaborating on repositories\nDemonstrate the ability to resolve conflicts using Git conflict resolution techniques\nApply workflows and best practices that minimize conflicts on collaborative repositories",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#introduction-to-git-and-github-tools-for-collaboration",
    "href": "session_02.html#introduction-to-git-and-github-tools-for-collaboration",
    "title": "Collaborating with Git and GitHub",
    "section": "2.1 Introduction to Git and GitHub Tools for Collaboration",
    "text": "2.1 Introduction to Git and GitHub Tools for Collaboration\n\n\n\nArtwork by Allison Horst\n\n\nGit is not only a powerful tool for individual work but also an excellent choice for collaborating with friends and colleagues. Git ensures that after you’ve completed your contributions to a repository, you can confidently synchronize your changes with changes made by others.\nOne of the easiest and most effective ways to collaborate using Git is by utilizing a shared repository on a hosting service like GitHub. This shared repository acts as a central hub, enabling collaborators to effortlessly exchange and merge their changes. With Git and a shared repository, you can collaborate seamlessly and work confidently, knowing that your changes will be integrated smoothly with those of your collaborators.\n\nThere are many advanced techniques for synchronizing Git repositories, but let’s start with a simple example.\nIn this example, the Collaborator will clone a copy of the Owner’s repository from GitHub, and the Owner will grant them Collaborator status, enabling the Collaborator to directly pull and push from the Owner’s GitHub repository.\n\n\n\n\n\nWe’ll be practicing the above workflow in the next exercises – here, a respository (aka repo) owner controls permissions on their remote repo, which is hosted on GitHub. They can push commits from their local repo to the remote repo. Similarly, they can pull commits from the remote repo to their cloned local repo(s) (remember, you can clone your repo to mulitple machines e.g. your laptop and your desktop). The repository owner adds a colleague as a collaborator by sending them an invite from the remote repo on GitHub. This collaborator can now push their own changes from their local repo to the now-shared remote repo (and also pull the Owner’s changes). Git and GitHub provide the tools for both colleagues to create and merge their changes to the shared remote repository.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "href": "session_02.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "title": "Collaborating with Git and GitHub",
    "section": "2.2 Collaborating with a trusted colleague without conflicts",
    "text": "2.2 Collaborating with a trusted colleague without conflicts\nWe start our collaboration by giving a trusted colleague access to our repository on GitHub. In this example, we define the Owner as the individual who owns the repository, and the Collaborator as the person whom the Owner chooses to give permission to make changes to their repository.\nThe Collaborator will make changes to the repository and then push those changes to the shared repository on GitHub. The Owner will then use pull to retrieve the changes without encountering any conflicts. This is the most ideal workflow.\nThe instructors will demonstrate this process in the next section.\n\nStep 0: Owner adds a Collaborator to their repository on GitHub\nThe Owner must change the settings of the remote repository and give the Collaborator access to the repository by inviting them as a collaborator. Once the Collaborator accepts the owner’s invitation, they will have push access to the repository – meaning they can contribute their own changes/commits to the Owner’s repository.\nTo do this, the owner will navigate to their remote repository on GitHub, then choose Settings &gt; Collaborators &gt; Add people, to send an email invitation. The invitation will show as “Pending” until accepted.\n\n\nStep 1: Collaborator clones the remote repository\nIn order to contribute, the Collaborator must clone the repository from the Owner’s GitHub account (Note: as a Collaborator, you won’t see the repository appear under your profile’s Repositories page). To do this, the Collaborator should navigate to the Owner’s repository on GitHub, then copy the clone URL. In RStudio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog box (see the earlier chapter introducing GitHub).\n\n\nINTERMEDIATE STEP: Collaborator communicates with Owner that they plan to make some changes\nFrequent communication is SO important when collaborating! Letting one another know that you’re about to make and push changes to the remote repo can help to prevent merge conflicts (and reduce headaches). The easiest way to avoid merge conflicts is to ensure that you and your collaborators aren’t working on the same file(s)/section(s) of code at the same time.\n\n\nStep 2: Collaborator edits files locally\nWith the repo cloned locally, the Collaborator can now make changes to the README.md file, adding a line or statement somewhere noticeable near the top. Save the changes.\n\n\nStep 3: Collaborator commits, pulls, and pushs\nIt’s recommended that all collaborators (including the repo Owner) follow this workflow when syncing changes between their local repo and the remote repo (in this example, the Collaborator is now following these steps):\n\nadd and commit your modified file(s) (e.g. the updated README.md)\npull to fetch and merge changes from the remote/origin repository (in an ideal situation, as we’re demonstrating here, any potential changes are merged seamlessly without conflict)\npush your changes to the remote/origin repository\n\n\n\n\n\n\n\nWhy do I need to add and commit files before pulling?\n\n\n\nRemember, git pull is a combination of git fetching remote changes to your local repo and git mergeing those changes from your local repo into your local working file(s).\nThe merge part of git pull will fail if you have uncommitted changes in your local working file(s) to avoid any potential overwriting of your own changes. Because of this, you should always, add/commit then pull, and finally push.\n\n\n\n\n\n\n\n\n\nINTERMEDIATE STEP: Collaborator communicates with Owner that they pushed their changes to GitHub\nRemember, communication is key! The Owner now knows that they can pull those changes down to their local repo.\n\n\nStep 4: Owner pulls new changes from the remote repo to their local repo\nThe Owner can now open their local working copy of the code in RStudio, and pull to fetch and merge those changes into their local copy.\nCongrats, the Owner now has your changes! Now, all three repositories – the remote/origin repository on GitHub, the Owner’s local repository, and the Collaborator’s local repository – should all be in the exact same state.\n\n\nINTERMEDIATE STEP: Owner communicates with Collaborator that they now plan to make some changes\nDid we mention that communication is important? :)\n\n\nStep 5: Owner edits, commits, pulls (just in case!) and pushes\nFollowing the same workflow as the Collaborator did earlier:\n\nadd and commit your modified file(s) (e.g. the updated README.md)\npull to fetch and merge changes from the remote/origin repository (in an ideal situation, as we’re demonstrating here, any potential changes are merged seamlessly without conflict)\npush your changes to the remote/origin repository\n\n\n\nINTERMEDIATE STEP: Owner communicates with Collaborator that they pushed their changes to GitHub\nYes, this seems silly to repeat, yet again – but it’s also easy to forget in practice!\n\n\nStep 6: Collaborator pulls new changes from the remote repo to their local repo\nThe Collaborator can now pull down those changes from the Owner, and all copies are once again fully synced. And just like that, you’ve successfully collaborated!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#ex1-no-conflict",
    "href": "session_02.html#ex1-no-conflict",
    "title": "Collaborating with Git and GitHub",
    "section": "2.3 Exercise 1: With a partner collaborate in a repository without a merge conflict",
    "text": "2.3 Exercise 1: With a partner collaborate in a repository without a merge conflict\n\n\n\n\n\n\nSetup\n\n\n\n\nGet into pairs, then choose one person as the Owner and one as the Collaborator\nBoth login to GitHub\n\nThese next steps are for the Owner:\n\nNavigate to the {FIRSTNAME}_test repository\nGo to Settings and navigate to Collaborators in the Access section on the left-hand side\nUnder Manage Access click the button Add people and type the username of your Collaborator in the search box\nOnce you’ve found the correct username, click Add {Collaborator username} to this repository\n\n\nNow, the Collaborator will follow this step:\n\nCheck your email for an invitation to GitHub or check your notifications (likely under Your Organizations) on GitHub to accept the invite to collaborate.\n\n\n\n\n2.3.1 Defining Merge Method\n\n\n\n\n\n\nSome Git configuration to surpress warning messages\n\n\n\nGit version 2.27 includes a new feature that allows users to specify the default method for integrating changes from a remote repository into a local repository, without receiving a warning (this warning is informative, but can get annoying). To suppress this warning for this repository only we need to configure Git by running this line of code in the Terminal:\n\ngit config pull.rebase false\n\npull.rebase false is a default strategy for pulling where Git will first try to auto-merge the files. If auto-merging is not possible, it will indicate a merge conflict.\nNote: Unlike when we first configured Git, we do not include the --global flag here (e.g. git config --global pull.rebase false). This sets this default strategy for this repository only (rather than globally for all your repositories). We do this because your chosen/default method of grabbing changes from a remote repository (e.g. pulling vs. rebasing) may change depending on collaborator/workflow preference.\n\n\n\n\n\n\n\n\nInstructions\n\n\n\nYou will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\n\nStep 0: Designate one person as the Owner and one as the Collaborator.\n\nRound One:\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository (see Setup block above for detailed steps)\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “Git Workflow”\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “Git Workflow”, Owner adds the steps of the Git workflow we’ve been practicing\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owners changes from GitHub\nStep 9: Go back to Step 0, switch roles, and then follow the steps in Round Two.\n\nRound Two:\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “How to Create a Git Repository from an existing project” and adds the high level steps for this workflow\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “How to Create a Git Repository”, Owner adds the high level steps for this workflow\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owners changes from GitHub\n\nHint: If you don’t remember how to create a Git repository, refer to the chapter Intro to Git and GitHub where we created two Git repositories",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#a-note-on-advanced-collaboration-techniques",
    "href": "session_02.html#a-note-on-advanced-collaboration-techniques",
    "title": "Collaborating with Git and GitHub",
    "section": "2.4 A Note on Advanced Collaboration Techniques",
    "text": "2.4 A Note on Advanced Collaboration Techniques\nThere are many Git and GitHub collaboration techniques, some more advanced than others. We won’t be covering advanced strategies in this course. But here is a table for your reference on a few popular Git collaboration workflow strategies and tools.\n\n\n\n\n\n\n\n\n\nCollaboration Technique\nBenefits\nWhen to Use\nWhen Not to Use\n\n\n\n\nBranch Management Strategies\n1. Enables parallel development and experimentation2. Facilitates isolation of features or bug fixes3. Provides flexibility and control over project workflows\nWhen working on larger projects with multiple features or bug fixes simultaneously.When you want to maintain a stable main branch while developing new features or resolving issues on separate branches.When collaborating with teammates on different aspects of a project and later integrating their changes.\nWhen working on small projects with a single developer or limited codebase.When the project scope is simple and doesn’t require extensive branch management.When there is no need to isolate features or bug fixes.\n\n\nCode Review Practices\n1. Enhances code quality and correctness through feedback2. Promotes knowledge sharing and learning within the team3. Helps identify bugs, improve performance, and ensure adherence to coding standards\nWhen collaborating on a codebase with team members to ensure code quality and maintain best practices.When you want to receive feedback and suggestions on your code to improve its readability, efficiency, or functionality.When working on critical or complex code that requires an extra layer of scrutiny before merging it into the main branch.\nWhen working on personal projects or small codebases with no collaboration involved.When time constraints or project size make it impractical to conduct code reviews.When the codebase is less critical or has low complexity.\n\n\nForking\n1. Enables independent experimentation and development2. Provides a way to contribute to a project without direct access3. Allows for creating separate, standalone copies of a repository\nWhen you want to contribute to a project without having direct write access to the original repository.When you want to work on an independent variation or extension of an existing project.When experimenting with changes or modifications to a project while keeping the original repository intact.\nWhen collaborating on a project with direct write access to the original repository.When the project does not allow external contributions or forking.When the project size or complexity doesn’t justify the need for independent variations.\n\n\nPull Requests\n1. Facilitates code review and discussion2. Allows for collaboration and feedback from team members3. Enables better organization and tracking of proposed changes\nWhen working on a shared repository with a team and wanting to contribute changes in a controlled and collaborative manner.When you want to propose changes to a project managed by others and seek review and approval before merging them into the main codebase.\nWhen working on personal projects or individual coding tasks without the need for collaboration.When immediate changes or fixes are required without review processes.When working on projects with a small team or single developer with direct write access to the repository.\n\n\n\nThe “When Not to Use” column provides insights into situations where it may be less appropriate / unnecessary to use each collaboration technique, helping you make informed decisions based on the specific context and requirements of your project.\nThese techniques provide different benefits and are used in various collaboration scenarios, depending on the project’s needs and team dynamics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#merge-conflicts",
    "href": "session_02.html#merge-conflicts",
    "title": "Collaborating with Git and GitHub",
    "section": "2.5 Merge conflicts",
    "text": "2.5 Merge conflicts\nMerge conflicts occur when both collaborators make conflicting changes to the same file. Resolving merge conflicts involves identifying the root of the problem and restoring the project to a normal state. Good communication, discussing file sections to work on, and avoiding overlaps can help prevent merge conflicts. However, if conflicts do arise, Git warns about potential issues and ensures that changes from different collaborators based on the same file version are not overwritten. To resolve conflicts, you need to explicitly specify whose changes should be used for each conflicting line in the file.\nIn this image, we see collaborators mbjones and metamattj have both made changes to the same line in the same README.md file. This is causing a merge conflict because Git doesn’t know whose changes came first. To resolve it, we need to tell Git whose changes to keep for that line, and whose changes to discard.\n\n\n2.5.1 Common ways to resolve a merge conflict\n1. Abort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the repository is placed in a “Merging” state until you resolve it. There’s a Terminal command to abort doing the merge altogether:\n\ngit merge --abort\n\nOf course, after doing that you still haven’t synced with your Collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine.\n2. Checkout\nThe simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line to tell Git to use either your changes (the person doing the merge), or their changes (the Collaborator).\n\nkeep your Collaborator’s file: git checkout --theirs conflicted_file.Rmd\nkeep your own file: git checkout --ours conflicted_file.Rmd\n\nOnce you have run that command, then run add (staging), commit, pull, and push the changes as normal.\n3. Pull and edit the file\nOption 2, above, requires the command line, however, we have a third option for resolving the merge conflict from RStudio. Using this approach will allow us to pick and choose some of our changes and some of our Collaborator’s changes by letting us manually edit and fix the conflicted file.\nWhen you pull a file with a conflict, Git will provide you with a warning modify the file so that it includes both your own changes and your Collaborator’s changes. The file will also appear in the Git tab with an orange U icon, which indicates that the file is Unmerged and therefore awaiting your help to resolve the conflict. It delimits these blocks of conflicted code with a series of less than and greater than signs, so they are easy to find:\n\n\n\n\n\nIn the above example, &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD marks the start of your changes. The ======= delimiter separates your changes from your Collaborator’s conflicting changes. &gt;&gt;&gt;&gt;&gt;&gt;&gt; mark the end of your Collaborator’s changes.\nTo resolve the conflicts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your Collaborator’s lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n\nOnce you have made those changes, you simply add (staging), commit, and push the files to resolve the conflict.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#producing-and-resolving-merge-conflicts",
    "href": "session_02.html#producing-and-resolving-merge-conflicts",
    "title": "Collaborating with Git and GitHub",
    "section": "2.6 Producing and resolving merge conflicts",
    "text": "2.6 Producing and resolving merge conflicts\nTo illustrate this process, the instructors are going to carefully create a merge conflict step-by-step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, the instructors will walk through the exercise to demonstrate the issues. Then, participants will pair up and try the exercise.\n\nStep 1: Owner and Collaborator ensure that their local repos are synced with the remote repo\nPair with the same partner as in Exercise 1 and decide who will be the Owner and who will be the Collaborator. Begin the exercise by ensuring that both the Owner and Collaborator have all of the changes synced from the remote repo to their local repos. This includes doing a git pull to ensure that you have all changes locally, and ensuring that the Git tab in RStudio doesn’t show any changes that need to be committed.\n\n\nStep 2: Owner makes a change and commits locally\nFrom this clean slate, the Owner will first modify and commit a small change. The Owner should add their name on a specific line of the README.md file (we will change the title in line 1). Save and commit the change (but DO NOT push). The Owner should now have a local but unpushed commit that the Collaborator does not yet have access to.\n\n\nStep 3: Collaborator makes a change and commits on the same line\nNow, the Collaborator will modify and commit a small change. The Collaborator should add their name to the same line of the README.md file (we will again change the title in line 1). Save and commit the change (but DO NOT push). The Collaborator should now also have a local but unpushed commit that the Owner does not yet have access to.\nAt this point, both the Owner and Collaborator have committed local changes, but neither have tried to share their changes via GitHub.\n\n\nStep 4: Collaborator pushes the file to GitHub\nSharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub with their version of the file. The Owner is now one revision behind, but doesn’t know it yet.\n\n\nStep 5: Owner pushes their changes and gets an error\nAt this point, the Owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it tells you everything needed (that the Owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push).\n\n\n\nStep 6: Owner pulls from GitHub to get Collaborator changes\nFollowing the error message, the Owner pulls the changes from GitHub, and gets another, different error message. Here, it indicates that there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange U, which stands for an unresolved merge conflict.\n\n\n\nStep 7: Owner edits the file to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, Git has flagged the locations in the file where a conflict occurred with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\n\nOf course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and that none of the logic of the program has been broken.\n\n\n\nStep 8: Owner commits the resolved changes\nFrom this point forward, things proceed as normal. The Owner first adds the file, which changes the orange U to a blue M for modified. Then, the Owner commits the changes locally. The Owner now has a resolved version of the file on their system.\n\n\n\nStep 9: Owner pushes the resolved changes to GitHub\nThe Owner can now push the changes, without error, to GitHub.\n\n\n\nStep 10: Collaborator pulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes (which include the resolved conflicted lines of code) that the Owner made.\n\n\nStep 11: Both can view commit history\nBoth the Collaborator and the Owner can view the history, which includes information about the conflict, the associated branch, and the merged changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "href": "session_02.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "title": "Collaborating with Git and GitHub",
    "section": "2.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict",
    "text": "2.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict\nNote you will only need to complete the Setup and Git configuration steps again if you are working in a new repository. Return to Exercise 1 for Setup and Git configuration steps.\n\n\n\n\n\n\nInstructions\n\n\n\nNow it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of the steps below. You will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\n\nStep 0: Designate one person as the Owner and one as the Collaborator.\n\nRound One:\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to the title and commits do not push\nStep 3: On the same line, Collaborator edits the README file and makes a change to the title and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history\nStep 12: Go back to Step 0, switch roles, and then follow the steps in Round Two.\n\nRound Two:\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to line 2 and commits do not push\nStep 3: On the same line, Collaborator edits the README file and makes a change to line 2 and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_02.html#best-practices-to-avoid-merge-conflicts",
    "href": "session_02.html#best-practices-to-avoid-merge-conflicts",
    "title": "Collaborating with Git and GitHub",
    "section": "2.8 Best practices to avoid merge conflicts",
    "text": "2.8 Best practices to avoid merge conflicts\nSome basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by:\n\n\n\n\n\nXKCD 1597\n\n\n\nCommunicate often and set up effective communication channels\nTell each other what you are working on\nStart your working session with a pull\nPull immediately after you commit and before you push\nCommit often in small chunks (this helps you organize your work!)\nMake sure you and who you are collaborating with all fully understand the Git workflow you’re using (aka make sure you’re on the same page before you start)!\n\nA good workflow is encapsulated as follows:\nPull -&gt; Edit -&gt; Save -&gt; Add (stage) -&gt; Commit -&gt; Pull -&gt; (OPTIONAL) Fix any merge conflicts -&gt; Push\nIt may take a bit of practice to get comfortable with navigating merge conflicts, but like any other technical skill, they’ll become less intimidating with time. With careful communication and a consistent workflow, conflicts can be largely avoided or resolved when they do occur.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "Git and GitHub",
    "section": "",
    "text": "Apply the principles of Git to track and manage changes of a project\nUtilize the Git workflow including pulling changes, staging modified files, committing changes, pulling again to incorporate remote changes, and pushing changes to a remote repository\nCreate and configure Git repositories using different workflows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#introduction-to-version-control",
    "href": "session_03.html#introduction-to-version-control",
    "title": "Git and GitHub",
    "section": "3.1 Introduction to Version Control",
    "text": "3.1 Introduction to Version Control\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when bugs are discovered. Sometimes those fixes lead to even more bugs, leading to more changes in the code base. Data files get combined together. Sometimes those same files are split and combined again. In just one research project, we can expect thousands of changes to occur.\nThese changes are important to track, and yet, we often use simplistic file names to do so. Many of us have experienced renaming a document or script multiple times with the disingenuous addition of “final” to the file name (like the comic above demonstrates).\nYou might think there is a better way, and you’d be right: version control. Version control provides an organized and transparent way to track changes in code and additional files. This practice was designed for software development, but is easily applicable to scientific programming.\nThere are many benefits to using a version control software including:\n\nMaintain a history of your research project’s development while keeping your workspace clean\nFacilitate collaboration and transparency when working on teams\nExplore bugs or new features without disrupting your team members’ work\nand more!\n\nThe version control system we’ll be diving into is Git, the most widely used modern version control system in the world.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#introduction-to-git-github",
    "href": "session_03.html#introduction-to-git-github",
    "title": "Git and GitHub",
    "section": "3.2 Introduction to Git + GitHub",
    "text": "3.2 Introduction to Git + GitHub\nBefore diving into the details of Git and how to use it, let’s start with a motivating example that’s representative of the types of problems Git can help us solve.\n\n3.2.1 A Motivating Example\nSay, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1:\n\n\n\nYou come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?”\n\n\n\nYou’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space.\nBut you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it.\n\n\n\nCommenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell Git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like.\n\n\n\nYou may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, Git also tracks who, when, and where the change was made.\nImagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead?\n\n\n\nMaybe you’re not so sure the idea will work out and this is where a tool like Git shines. Without a tool like Git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to.\nInstead, with Git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code in tact and recoverable.\n\n\n\nSo you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like Git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method.\n\n\n\nBut with Git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally.\n\n\n\nAfter all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up.\n\n\n\nIf, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful.\n\n\n\nA key takeaway here is that Git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this.\n\n\n\nFinally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of Git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past.\n\nWith Git we can enhance our workflow:\n\nEliminate the need for cryptic filenames and comments to track our work.\nProvide detailed descriptions of our changes through commits, making it easier to understand the reasons behind code modifications.\nWork on multiple branches simultaneously, allowing for parallel development, and optionally merge them together.\nUse commits to access and even execute older versions of our code.\nAssign meaningful tags to specific versions of our code.\nAdditionally, Git offers a powerful distributed feature. Multiple individuals can work on the same analysis concurrently on their own computers, with the ability to merge everyone’s changes together.\n\n\n\n\n3.2.2 What exactly are Git and GitHub?\n\nGit:\n\nan open-source distributed version control software\ndesigned to manage the versioning and tracking of source code files and project history\noperates locally on your computer, allowing you to create repositories, and track changes\nprovides features such as committing changes, branching and merging code, reverting to previous versions, and managing project history\nworks directly with the files on your computer and does not require a network connection to perform most operations\nprimarily used through the command-line interface (CLI, e.g. Terminal), but also has various GUI tools available (e.g. RStudio IDE)\n\n\n\n\n\n\nGitHub:\n\nonline platform and service built around Git\nprovides a centralized hosting platform for Git repositories\nallows us to store, manage, and collaborate on their Git repositories in the cloud\noffers additional features on top of Git, such as a web-based interface, issue tracking, project management tools, pull requests, code review, and collaboration features\nenables easy sharing of code with others, facilitating collaboration and contribution to open source projects\nprovides a social aspect, allowing users to follow projects, star repositories, and discover new code\n\n\n\n\n\n\n\n3.2.3 Understanding how local working files, Git, and GitHub all work together\nIt can be a bit daunting to understand all the moving parts of the Git / GitHub life cycle (i.e. how file changes are tracked locally within repositories, then stored for safe-keeping and collaboration on remote repositories, then brought back down to a local machine(s) for continued development). It gets easier with practice, but we’ll explain (first in words, then with an illustration) at a high-level how things work:\n\n3.2.3.1 What is the difference between a “normal” folder vs. a Git repository\nWhether you’re a Mac or a PC user, you’ll likely have created a folder at some point in time for organizing files. Let’s pretend that we create a folder, called myFolder/, and add two files: myData.csv and myAnalysis.R. The contents of this folder are not currently version controlled – meaning, for example, that if we make some changes to myAnalysis.R that don’t quite work out, we have no way of accessing or reverting back to a previous version of myAnalysis.R (without remembering/rewriting things, of course).\nGit allows you to turn any “normal” folder, like myFolder/, into a Git repository – you’ll often see/hear this referenced as “initializing a Git repository”. When you initialize a folder on your local computer as a Git repository, a hidden .git/ folder is created within that folder (e.g. myFolder/.git/) – this .git/ folder is the Git repository. As you use Git commands to capture versions or “snapshots” of your work, those versions (and their associated metadata) get stored within the .git/ folder. This allows you to access and/or recover any previous versions of your work. If you delete .git/, you delete your project’s history.\nHere is our example folder / Git repository represented visually:\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3.2 How do I actually tell Git to preserve versions of my local working files?\nGit was built as a command-line tool, meaning we can use Git commands in the command line (e.g. Terminal, Git Bash, etc.) to take “snapshots” of our local working files through time. Alternatively, RStudio provides buttons that help to easily execute these Git commands.\nGenerally, that workflow looks something like this:\n\nMake changes to a file(s) (e.g. myAnalysis.R) in your working directory.\nStage the file(s) using git add myAnalysis.R (or git add . to stage multiple changed files at once). This lets Git know that you’d like to include the file(s) in your next commit.\nCommit the file(s) using git commit -m \"a message describing my changes\". This records those changes (along with a descriptive message) as a “snapshot” or version in the local repository (i.e. the .git/ folder).\n\n\n\n3.2.3.3 My versioned work is on my local computer, but I want to send it to GitHub. How?\nThe last step is synchronizing the changes made to our local repository with a remote repository (oftentimes, this remote repository is stored on GitHub). The git push command is used to send local commits up to a remote repository. The git pull command is used to fetch changes from a remote repository and merge them into the local repository – pulling will become a regular part of your workflow when collaborating with others, or even when working alone but on different machines (e.g. a laptop at home and a desktop at the office).\nThe processes described in the above sections (i.e. making changes to local working files, recording “snapshots” of them to create a versioned history of changes in a local Git repository, and sending those versions from our local Git repository to a remote repository (which is oftentimes on GitHub)) is illustrated using islands, buildings, bunnies, and packages in the artwork, below:\nA basic git workflow represented as two islands, one with “local repo” and “working directory”, and another with “remote repo.” Bunnies move file boxes from the working directory to the staging area, then with Commit move them to the local repo. Bunnies in rowboats move changes from the local repo to the remote repo (labeled “PUSH”) and from the remote repo to the working directory (labeled “PULL”).\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n\n3.2.4 Let’s Look at a GitHub Repository\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\n\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson was working on the project and fixing errors in December:\n\n\n\nAnd finally, if we drill into one of the changes made on December 20, we can see exactly what was changed in each file:\n\n\n\nTracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow.\n\n\n3.2.5 Git Vocabulary & Commands\nWe know the world of Git and GitHub can be daunting. Use these tables as references while you use Git and GitHub, and we encourage you to build upon this list as you become more comfortable with these tools.\nThis table contains essential terms and commands that complement intro to Git skills. They will get you far on personal and individual projects.\n\nEssential Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nAdd/Stage\ngit add [file]\nStaging marks a modified file in its current version to go into your next commit snapshot. You can also stage all modified files at the same time using git add .\n\n\nCommit\ngit commit\nRecords changes to the repository.\n\n\nCommit Message\ngit commit -m \"my commit message\"\nRecords changes to the repository and include a descriptive message (you should always include a commit message!).\n\n\nFetch\ngit fetch\nRetrieves changes from a remote repository but does not merge them into your local working file(s).\n\n\nPull\ngit pull\nRetrieves changes from a remote repository and merges them into your local working file(s).\n\n\nPush\ngit push\nSends local commits to a remote repository.\n\n\nStatus\ngit status\nShows the current status of the repository, including (un)staged files and branch information.\n\n\n\nThis table includes more advanced Git terms and commands that are commonly used in both individual and collaborative projects.\n\nAdvanced Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nBranch\ngit branch\nLists existing branches or creates a new branch.\n\n\nCheckout\ngit checkout [branch]\nSwitches to a different branch or restores files from a specific commit.\n\n\nClone\ngit clone [repository]\nCreates a local copy of a remote repository.\n\n\nDiff\ngit diff\nShows differences between files, commits, or branches.\n\n\nFork\n-\nCreates a personal copy of a repository under your GitHub account for independent development.\n\n\nLog\ngit log\nDisplays the commit history of the repository.\n\n\nMerge\ngit merge [branch]\nIntegrates changes from one branch into another branch.\n\n\nMerge Conflict\n-\nOccurs when Git cannot automatically merge changes from different branches, requiring manual resolution.\n\n\nPull Request (PR)\n-\nA request to merge changes from a branch into another branch, typically in a collaborative project.\n\n\nRebase\ngit rebase\nIntegrates changes from one branch onto another by modifying commit history.\n\n\nRemote\ngit remote\nManages remote repositories linked to the local repository.\n\n\nRepository\ngit init\nA directory where Git tracks and manages files and their versions.\n\n\nStash\ngit stash\nTemporarily saves changes that are not ready to be committed.\n\n\nTag\ngit tag\nAssigns a label or tag to a specific commit.\n\n\n\nGit has a rich set of commands and features, and there are many more terms beyond either table. Learn more by visiting the git documentation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#exercise-1-create-a-remote-repository-on-github",
    "href": "session_03.html#exercise-1-create-a-remote-repository-on-github",
    "title": "Git and GitHub",
    "section": "3.3 Exercise 1: Create a remote repository on GitHub",
    "text": "3.3 Exercise 1: Create a remote repository on GitHub\n\n\n\n\n\n\nSetup\n\n\n\n\nLogin to GitHub\nClick the New repository button\nName it {FIRSTNAME}_test\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nIf you were successful, it should look something like this:\n\n\n\n\n\nYou’ve now created your first repository! It has a couple of files that GitHub created for you: README.md, LICENSE, and .gitignore.\n\n\n\n\n\n\nREADME.md files are used to share important information about your repository\n\n\n\nYou should always add a README.md to the root directory of your repository – it is a markdown file that is rendered as HTML and displayed on the landing page of your repository. This is a common place to include any pertinent information about what your repository contains, how to use it, etc.\n\n\n\n\n \n\nFor simple changes to text files, such as the README.md, you can make edits directly in the GitHub web interface.\n\n\n\n\n\n\nChallenge\n\n\n\nNavigate to the README.md file in the file listing, and edit it by clicking on the pencil icon (top right of file). This is a regular Markdown file, so you can add markdown text. Add a new level-2 header called “Purpose” and add some bullet points describing the purpose of the repo. When done, add a commit message, and hit the Commit changes button.\n\n\n\n\n\n\n\nCongratulations, you’ve now authored your first versioned commit! If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file.\n\n\n\n\n\nThe GitHub repository landing page provides us with lots of useful information. To start, we see:\n\nall of the files in the remote repository\nwhen each file was last edited\nthe commit message that was included with each file’s most recent commit (which is why it’s important to write good, descriptive commit messages!)\n\nAdditionally, the header above the file listing shows the most recent commit, along with its commit message, and a unique ID (assigned by Git) called a SHA. The SHA (aka hash) identifies the specific changes made, when they were made, and by who. If you click on the SHA, it will display the set of changes made in that particular commit.\n\n\n\n\n\n\nWhat should I write in my commit message?\n\n\n\nWriting effective Git commit messages is essential for creating a meaningful and helpful version history in your repository. It is crucial to avoid skipping commit messages or resorting to generic phrases like “Updates.” When it comes to following best practices, there are several guidelines to enhance the readability and maintainability of the codebase.\nHere are some guidelines for writing effective Git commit messages:\n\nBe descriptive and concise: Provide a clear and concise summary of the changes made in the commit. Aim to convey the purpose and impact of the commit in a few words.\nUse imperative tense: Write commit messages in the imperative tense, as if giving a command. For example, use “Add feature” instead of “Added feature” or “Adding feature.” This convention aligns with other Git commands and makes the messages more actionable.\nSeparate subject and body: Start with a subject line, followed by a blank line, and then provide a more detailed explanation in the body if necessary. The subject line should be a short, one-line summary, while the body can provide additional context, motivation, or details about the changes.\nLimit the subject line length: Keep the subject line within 50 characters or less. This ensures that the commit messages are easily scannable and fit well in tools like Git logs.\nCapitalize and punctuate properly: Begin the subject line with a capital letter and use proper punctuation. This adds clarity and consistency to the commit messages.\nFocus on the “what” and “why”: Explain what changes were made and why they were made. Understanding the motivation behind a commit helps future researchers and collaborators (including you!) comprehend its purpose.\nUse present tense for subject, past tense for body: Write the subject line in present tense as it represents the current state of the codebase. Use past tense in the body to describe what has been done.\nReference relevant issues: If the commit is related to a specific issue or task, include a reference to it. For example, you can mention the issue number or use keywords like “Fixes,” “Closes,” or “Resolves” followed by the issue number.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "href": "session_03.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "title": "Git and GitHub",
    "section": "3.4 Exercise 2: clone your repository and use Git locally in RStudio",
    "text": "3.4 Exercise 2: clone your repository and use Git locally in RStudio\nCurrently, our repository just exists on GitHub as a remote repository. It’s easy enough to make changes to things like our README.md file (as demonstrated above), from the web browser, but that becomes a lot harder (and discouraged) for scripts and other code files. In this exercise, we’ll bring a copy of this remote repository down to our local computer (aka clone this repository) so that we can work comfortably in RStudio.\n\n\n\n\n\n\nAn important distinction\n\n\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local repository.\n\n\nStart by clicking the green Code button (top right of your file listing) and copying the URL to your clipboard (this URL represents the repository location):\n\n\n\n\n\n\n\nRStudio makes working with Git and version controlled files easy – to do so, you’ll need to be working within an R project folder. The following steps will look similar to those you followed when first creating an R Project, with a slight difference. Follow the instructions in the Setup box below to clone your remote repository to your local computer in RStudio:\n\n\n\n\n\n\nSetup\n\n\n\n\nClick File &gt; New Project\nSelect Version Control and paste the remote repository URL (which should be copied to your clipboard) in the Repository URL field\nPress Tab, which will auto-fill the Project directory name field with the same name as that of your remote repo – while you can name the local copy of the repository anything, it’s typical (and highly recommended) to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\n\nOnce you click Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present – you should see a Git tab, as well as the Files tab, where you can view all of the files copied from the remote repo to this local repo.\n\n\n\n\nYou’ll note that there is one new file sam_test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\nIn the Git tab, you’ll note that the one new file, sam_test.Rproj, is listed. This Git tab is the status pane that shows the current modification status of all of the files in the repository. Here, we see sam_test.Rproj is preceded by a ?? symbol to indicate that the file is currently untracked by Git. This means that we have not yet committed this file using Git (i.e. Git knows nothing about the file; hang tight, we’ll commit this file soon so that it’s tracked by Git). As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files.\nInspect the history. Click on the History button in the Git tab to show the log of changes that have occurred – these changes will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nMake a change to the README.md file – this time from RStudio – then commit the README.md change\nAdd a new section to your README.md called “Creator” using a level-2 header. Under it include some information about yourself. Bonus: Add some contact information and link your email using Markdown syntax.\n\n\n\nOnce you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. Select the file in the Git tab, and click Diff to see the changes that you saved (but which are not yet committed to your local repository). Newly made changes are highlighted in green.\n\n\n\n\nCommit the changes. To commit the changes you made to the README.md file using RStudio’s GUI (Graphical User Interface), rather than the command line:\n\nStage (aka add) README.md by clicking the check box next to the file name – this tells Git which changes you want included in the commit and is analogous to using the git command, git add README.md, in the command line\nCommit README.md by clicking the Commit button and providing a descriptive commit message in the dialog box. Press the Commit button once you’re satisfied with your message. This is analogous to using the git command, git commit -m \"my commit message\", in the command line.\n\n\nA few notes about our local repository’s state:\n\nWe still have a file, sam_test.Rproj, that is listed as untracked (denoted by ?? in the Git tab).\nYou should see a message at the top of the Git tab that says, Your branch is ahead of ‘origin/main’ by 1 commit., which tells us that we have 1 commit in the local repository, but that commit has not yet been pushed up to the origin repository (aka remote repository on GitHub).\n\nCommit the remaining project file by staging/adding and committing it with an informative commit message.\n\nWhen finished, you’ll see that no changes remain in the Git tab, and the repository is clean.\nInspect the history. Note that under Changes, the message now says:\nYour branch is ahead of ‘origin/main’ by 2 commits.\nThese are the two commits that we just made, but have not yet been pushed to GitHub.\nClick on the History button to see a total of four commits in the local repository (the two we made directly to GitHub via the web browser and the two we made in RStudio).\nPush these changes to GitHub. Now that we’ve made and committed changes locally, we can push those changes to GitHub using the Push button. This sends your changes to the remote repository (on GitHub) leaving your repository in a totally clean and synchronized state (meaning your local repository and remote repository should look the same).\n\n\n\n\n\n\nIf you are prompted to provide your GitHub username and password when Pushing…\n\n\n\nit’s a good indicator that you did not set your GitHub Personal Access Token (PAT) correctly. You can redo the steps outlined in the GitHub Authentication section to (re)set your PAT, then Push again.\n\n\n\n &lt;––&gt;\n\nIf you look at the History pane again, you’ll notice that the labels next to the most recent commit indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. If we look at the commit history on GitHub, all the commits will be shown there as well.\n\n\n\n3.4.1 Defining Merge Method\n\n\n\n\n\n\nSome Git configuration to surpress warning messages\n\n\n\nGit version 2.27 includes a new feature that allows users to specify the default method for integrating changes from a remote repository into a local repository, without receiving a warning (this warning is informative, but can get annoying). To suppress this warning for this repository only we need to configure Git by running this line of code in the Terminal:\n\ngit config pull.rebase false\n\npull.rebase false is a default strategy for pulling where Git will first try to auto-merge the files. If auto-merging is not possible, it will indicate a merge conflict (more on resolving merge conflicts in Collaborating with Git and GitHub).\nNote: Unlike when we first configured Git, we do not include the --global flag here (e.g. git config --global pull.rebase false). This sets this default strategy for this repository only (rather than globally for all your repositories). We do this because your chosen/default method of grabbing changes from a remote repository (e.g. pulling vs. rebasing) may change depending on collaborator/workflow preference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#exercise-3-setting-up-git-on-an-existing-project",
    "href": "session_03.html#exercise-3-setting-up-git-on-an-existing-project",
    "title": "Git and GitHub",
    "section": "3.5 Exercise 3: Setting up Git on an existing project",
    "text": "3.5 Exercise 3: Setting up Git on an existing project\nThere are a number of different workflows for creating version-controlled repositories that are stored on GitHub. We started with Exercise 1 and Exercise 2 using one common approach: creating a remote repository on GitHub first, then cloning that repository to your local computer (you used your {FIRSTNAME}_test repo).\nHowever, you may find yourself in the situation where you have an existing directory (i.e. a “normal” folder) of code that you want to make a Git repository out of, and then send it to GitHub. In this last exercise, we will practice this workflow using your training_{USERNAME} project.\nFirst, switch to your training_{USERNAME} project using the RStudio project dropdown menu. The project drop down menu is in the upper right corner of your RStudio pane. Click the drop down next to your project name ({FIRSTNAME}_test), and then select the training_{USERNAME} project from the RECENT PROJECTS list.\nThere are a few approaches for turning an existing project folder into a Git repository, then sending it to GitHub – if you’re an R-user, the simplest way is to use the {usethis} package, which is built to automate tasks involved with project setup and development. However, you can also initialize a local git repository and set the remote repository from the command line (a language-agnostic workflow). Steps for both approaches are included below (demonstrated using your training_{USERNAME} project):\n\nUsing R & {usethis}Using the command line\n\n\n\nInstall the {usethis} package (if you haven’t done so already) by running the following in your Console:\n\n\ninstall.packages(\"usethis\")\n\n\nInitialize training_{USERNAME} as a Git repository by running usethis::use_git() in the Console. Choose yes when asked if it’s okay to commit any uncommitted files. Choose yes again if asked to restart R. Once complete, you should see the Git tab appear in your top left pane in RStudio and a .gitignore file appear in your Files tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.gitignore files allow you to specify which files/folders you don’t want Git to track\n\n\n\nA .gitignore file is automatically created in the root directory of your project when you initialize it as a Git repository. You’ll notice that there are already some R / R Project-specific files that have been added by default.\nWhy is this useful? For many reasons, but possibly the greatest use-case is adding large files (GitHub has a file size limit of 2 GB) or files with sensitive information (e.g. keys, tokens) that you don’t want to accidentally push to GitHub.\nHow do I do this? Let’s say I create a file with sensitive information that I don’t want to push to GitHub. I can add a line to my .gitignore file:\n\n# added by default when I initalized my RProj as a Git Repository\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\n.quarto\n\n# add file so that it doesn't get pushed to the remote repo (on GitHub); \ncontains_sensitive_info.R\n\nIf this file is currently untracked by Git, it should appear in my Git tab. Once I add it to the .gitignore and save the modified .gitignore file, you should see contains_sensitive_info.R disappear from the Git tab, and a modified .gitignore (denoted by a blue M) appear. Stage/commit/push this modified .gitignore file.\n\n\n\nCreate an upstream remote repository on GitHub by running usethis::use_github() in the Console. Your web browser should open up to your new GitHub repository, with the same name as your local Git repo/R Project.\n\n\n\n\n\n\n\n\n\n\n\nEnsure that your default branch is named main rather than master by:\n\nrunning git branch in the Terminal to list all your branches (you should currently only have one, which is your default)\nif it’s named master, run the following line in the Console to update it\n\n\n\nusethis::git_default_branch_rename(from = \"master\", to = \"main\")\n\nYou can verify that your update worked by running git branch once more in the Terminal.\n\n\n\n\n\n\nWhy are we doing this?\n\n\n\nThe racist “master” terminology for git branches motivates us to update our default branch to “main” instead.\nThere is a push across platforms and software to update this historical default branch name from master to main. GitHub has already done so – you may have noticed that creating a remote repository first (like we did in Exercises 1 & 2) results in a default branch named main. Depending on your version of Git, however, you may need to set update the name manually when creating a local git repository first (as we’re doing here).\n\n\n\nYou’re now ready to edit, stage/add, commit, and push files to GitHub as practiced earlier!\n\n\n\n\n\n\n\nChallenge: add a README.md file to training_{USERNAME}\n\n\n\nGitHub provides a button on your repo’s landing page for quickly adding a README.md file. Click the Add a README button and use markdown syntax to create a README.md. Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made.\n\n\n\n\nWhile we’ll be using the RStudio Terminal here, you can use any command-line interface (e.g. Mac Terminal, Git Bash, etc.) that allows for git interactions (if you plan to use a command-line interface that is not the RStudio Terminal, make sure to navigate to your project directory (e.g. using cd file/path/to/project/directory) before initializing your repository.\n\nInitialize training_{USERNAME} as a Git repository by running git init in the Terminal. You should get a message that says something like:\n\n\nInitialized empty Git repository in /home/username/training_username/.git/\n\n\n\n\n\n\n\nYou may have to quit and reopen your RStudio session on the server for the Git tab to appear\n\n\n\nYou’ll likely need to help included-crab along in recognizing that this R Project has been initialized as a git repository – click Session &gt; Quit Session… &gt; New Session &gt; choose training_{USERNAME} to reopen your project.\n\n\nOnce complete, you should see the Git tab appear in your top left pane in RStudio and a .gitignore file appear in your Files tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.gitignore files allow you to specify which files/folders you don’t want Git to track\n\n\n\nA .gitignore file is automatically created in the root directory of your project when you initialize it as a Git repository. You’ll notice that there are already some R / R Project-specific files that have been added by default.\nWhy is this useful? For many reasons, but possibly the greatest use-case is adding large files (GitHub has a file size limit of 2 GB) or files with sensitive information (e.g. keys, tokens) that you don’t want to accidentally push to GitHub.\nHow do I do this? Let’s say I create a file with sensitive information that I don’t want to push to GitHub. I can add a line to my .gitignore file:\n\n# added by default when I initalized my RProj as a Git Repository\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\n.quarto\n\n# add file so that it doesn't get pushed to the remote repo (on GitHub); \ncontains_sensitive_info.R\n\nIf this file is currently untracked by Git, it should appear in my Git tab. Once I add it to the .gitignore and save the modified .gitignore file, you should see contains_sensitive_info.R disappear from the Git tab, and a modified .gitignore (denoted by a blue M) appear. Stage/commit/push this modified .gitignore file.\n\n\n\nEnsure that your default branch is named main rather than master by:\n\nrunning git branch in the Terminal to list all your branches (you should currently only have one, which is your default)\nif it’s named master, run the following line in the Terminal to update it\n\n\n\n# for Git version 2.28+ (check by running `git --version`)\n# this sets the default branch name to `main` for any new repos moving forward\ngit config --global init.defaultBranch main\n\n# for older versions of Git\n# this sets the default branch name to `main` ONLY for this repo \ngit branch -m master main\n\nYou can verify that your update worked by running git branch once more in the Terminal.\n\n\n\n\n\n\nWhy are we doing this?\n\n\n\nThe racist “master” terminology for git branches motivates us to update our default branch to “main” instead.\nThere is a push across platforms and software to update this historical default branch name from master to main. GitHub has already done so – you may have noticed that creating a remote repository first (like we did in Exercises 1 & 2) results in a default branch named main. Depending on your version of Git, however, you may need to set update the name manually when creating a local git repository first (as we’re doing here).\n\n\n\nStage/Add your files. It’s helpful to first run git status to check the state of your local repository (particularly if you aren’t using RStudio / have access to a GUI with a Git tab-esque feature) – this will tell you which files have been modified or are untracked and that are currently unstaged (in red). What appears here should look just like what appears in the Git tab:\n\n\n\n\n\n\n\n\n\n\nRun git add . in the Terminal to stage all files at once (or git add {FILENAME} to stage individual files). Running git status again will show you which files have been staged (in green). You may have to refresh your Git tab to see the change in state reflected in the GUI.\n\n\n\n\n\n\n\n\n\n\nCommit your files by running git commit -m \"an informative commit message\" in the Terminal. Refreshing your Git tab will cause them to disappear (just as they do when you commit using RStudio’s GUI buttons). You can run git log in the Terminal to see a history of your past commits (currently, we only have this one).\n\n\n\n\n\n\n\n\n\n\n\nCreate an empty remote repository by logging into GitHub and creating a new repository, following the same steps as in Exercise 1. IMPORTANTLY, DO NOT initialize your remote repo with a README license, or .gitignore file – doing so now can lead to merge conflicts. We can add them after our local and remote repos are linked. Name your remote repository the same as your local repository (i.e. training_{USERNAME}).\nLink your remote (GitHub) repository to your local Git repository. Your empty GitHub repo conveniently includes instructions for doing so. Copy the code under push an existing repository from the command line to your clipboard, paste into your RStudio Terminal, and press return/enter.\n\n\n\n\n\n\n\n\n\n\nThese commands do three things:\n\nAdds the GitHub repository as the remote repository (i.e. links your local repo to the remote repo)\nRenames the default branch to main\nPushes the main branch to the remote GitHub repository\n\nHead back to your browser and refresh your GitHub repository page to see your files appear!\n\nYou’re now ready to edit, stage/add, commit, and push files to GitHub as practiced earlier!\n\n\n\n\n\n\n\nChallenge: add a README.md file to training_{USERNAME}\n\n\n\nGitHub provides a button on your repo’s landing page for quickly adding a README.md file. Click the Add a README button and use markdown syntax to create a README.md. Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#go-further-with-git",
    "href": "session_03.html#go-further-with-git",
    "title": "Git and GitHub",
    "section": "3.6 Go further with Git",
    "text": "3.6 Go further with Git\nThere’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as:\n\nUsing Git on the command line\nResolving conflicts\nBranching and merging\nPull requests versus direct contributions for collaboration\nUsing .gitignore to protect sensitive data\nGitHub Issues - how to use them for project management and collaboration\n\nand much, much more.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_03.html#git-resources",
    "href": "session_03.html#git-resources",
    "title": "Git and GitHub",
    "section": "3.7 Git resources",
    "text": "3.7 Git resources\n\nPro Git Book\nHappy Git and GitHub for the useR\nGitHub Documentation\nLearn Git Branching is an interactive tool to learn Git on the command line\nSoftware Carpentry Version Control with Git\nBitbucket’s tutorials on Git Workflows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "Tidy Data",
    "section": "",
    "text": "Learning Objectives\nLearn how to design and create effective data tables by:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "Tidy Data",
    "section": "",
    "text": "applying tidy and normalized data principles,\nfollowing best practices to format data tables’ content,\nrelating tables following relational data models principles, and\nunderstanding how to perform table joins.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#what-is-tidy-data",
    "href": "session_04.html#what-is-tidy-data",
    "title": "Tidy Data",
    "section": "4.1 What is tidy data?",
    "text": "4.1 What is tidy data?\nTidy data is a standardized way of organizing data tables that allows us to manage and analyze data efficiently, because it makes it straightforward to understand the corresponding variable and observation of each value. The  tidy data principles  are:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n\n\n\n\n\nTidy Data: A way of life\n\n\n\n\nTidy data is not a language or tool specific.\nTidy data is not an R thing.\nTidy data is not a tidyverse thing.\n\nTidy Data is a way to organize data that will make life easier for people working with data.\n(Allison Horst & Julia Lowndes)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#values-variables-observations-and-entities",
    "href": "session_04.html#values-variables-observations-and-entities",
    "title": "Tidy Data",
    "section": "4.2 Values, variables, observations, and entities",
    "text": "4.2 Values, variables, observations, and entities\nFirst, let’s get acquainted with our building blocks.\n\n\n\n\n\n\n\n\n\nConcept\nDefinition\n\n\n\n\n\n\nVariables\nA characteristic the is being measured, counted or described with data.\nExample: car type, salinity, year, height, mass.\n\n\n\n\nObservations\nA single “data point” for which the measure, count or description of one or more variables is recorded.\nExample: If you are collecting variables height, species, and location of plants, then each plant is an observation.\n\n\n\n\nValue\nThe record measured, count or description of a variable.\nExample: 3 ft\n\n\n\n\nEntity\nEach type of observation is an entity.\nExample: If you are collecting variables height, species, and location and site name of plants and where they are observed, then plants is an entity and site is an entity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#assessing-tidy-data-principles",
    "href": "session_04.html#assessing-tidy-data-principles",
    "title": "Tidy Data",
    "section": "4.3 Assessing Tidy Data Principles",
    "text": "4.3 Assessing Tidy Data Principles\nThe following is an example of tidy data - it’s easy to see the three tidy data principles apply.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#recognizing-untidy-data",
    "href": "session_04.html#recognizing-untidy-data",
    "title": "Tidy Data",
    "section": "4.4 Recognizing untidy data",
    "text": "4.4 Recognizing untidy data\nAnything that does not follow the three tidy data principles is untidy data.\nThere are many ways in which data can become untidy, some can be noticed right away, while others are more subtle. In this section we will look at some examples of common untidy data situations.\n\n4.4.1 Example 1\nThe following is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper into why we consider it untidy data.\n\n\n4.4.1.1 Multiple tables\nTo begin with, notice there are actually three smaller tables within this table. Although for our human brain it is easy to see and interpret this, it is extremely difficult to get a computer to see it this way.\n\nHaving multiple tables within the same table will create headaches down the road should you try to read in this information using R or another programming language. Having multiple tables immediately breaks the tidy data principles, as we will see next.\n\n\n4.4.1.2 Inconsistent columns\nIn tidy data, each column corresponds to a single variable. If you look down a column, and see that multiple variables exist in the table, the data is not tidy. A good test for this can be to see if you think the column consists of only one unit type.\n\n\n\n4.4.1.3 Inconsistent rows\nThe second principle of tidy data is: every column must be a single observation. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy.\n\n\n\n4.4.1.4 Marginal sums and statistics\nMarginal sums and statistics are not considered tidy. They break principle one, “Every column is a variable”, because a marginal statistic does not represent the same variable as the values it is summarizing. They also break principle two, “Every row is an observation”, because they represent a combination of observations, rather than a single one.\n\n\n\n\n4.4.2 Example 2\nConsider the following table. It’s a single one this time! It shows data about species observed at a specific site and date. The column headers refer to the following:\n\ndate: date when a species was observed\nsite: site where a species was observed\nname: site’s name\naltitude: site’s altitude\nsp1code, sp2code: species code for two plants observed\nsp1height, sp2height: height of the plants observed\n\nTake a moment to see why this is not tidy data.\n\n\n4.4.2.1 Multiple Observations\nRemember that an observation is all the values measured for an individual entity.\nIf our entity is a single observed plant, then the values we measured are date and site of observation, the altitude, and the species code and height. This table breaks the second tidy data principles: Every row is an observation.\n\nPeople often refer to this as “wide format”, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain. To solve this problem, we can create a single column for species code and a single column for species height as in the following table.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#why-tidy-data",
    "href": "session_04.html#why-tidy-data",
    "title": "Tidy Data",
    "section": "4.5 Why Tidy Data?",
    "text": "4.5 Why Tidy Data?\n\nEfficiency: less re-creating the wheel. Easier to apply the same tools to different datasets.\nCollaboration: Makes it easier to work with others as you can work with the same tools in the same ways.\nReuse: It makes it easier to apply similar techniques and analysis across different or new datasets.\nGeneralizability: Tools built for one tidy data set can be used to multiple other datasets. Opening posibilities of data you can work with.\n\n\n“There is a specific advantage to placing varables in columns becasuse it allows R’s vectorized nature to shine. …most buit-in R functions work with vactors of values. That makes transforming tidy data feel particularly natural. (R for Data Science by Grolemund and Wickham)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#data-normalization",
    "href": "session_04.html#data-normalization",
    "title": "Tidy Data",
    "section": "4.6 Data Normalization",
    "text": "4.6 Data Normalization\n\n4.6.1 What is data normalization?\nData normalization is the process of creating normalized data, which are datasets free from data redundancy to simplify query, analysis, storing, and maintenance. In normalized data we organize data so that :\n\nEach table follows the tidy data principles\nWe have separate tables for each type of entity measured\nObservations (rows) are all unique\nEach column represents either an identifying variable or a measured variable\n\nIn denormalized data observations about different entities are combined. A good indication that a data table is denormalized and needs normalization is seeing the same column values repeated across multiple rows.\n\n\n4.6.2 Example\nIn the previous data table the row values for the last three columns are repeated.\n\nThis means the data is denormalized and it happens because each row has values about more than one entity:\n\n1st entity: individual plants found at that site, and\n2nd entity: sites at which the plants were observed.\n\n\nIf we use this information to normalize our data, we should end up with:\n\none tidy table for each entity observed, and\nadditional columns for identifying variables (such as site ID).\n\nHere’s how our normalized data would look like:\n\n\n\n\n\nNotice that each table also satisfies the tidy data principles.\nNormalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and plant information are in separate tables, how would we use site altitude as a predictor variable for species composition, for example? We will go over a solution in the next section.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#relational-data-models",
    "href": "session_04.html#relational-data-models",
    "title": "Tidy Data",
    "section": "4.7 Relational Data Models",
    "text": "4.7 Relational Data Models\n\nIt’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. (R for Data Science Chapter 13 Relational Data)\n\n\n4.7.1 What are relational data models?\nA relational data model is a way of encoding links between multiple tables in a database. A database organized following a relational data model is a relational database. A few of the advantages of using a relational data model are:\n\nEnabling powerful search and filtering\nAbility to handle large, complex data sets\nEnforcing data integrity\nDecreasing errors from redundant updates\n\nRelational data models are used by relational databases (like mySQL, MariaDB, Oracle, or Microsoft Access) to organize tables. However, you don’t have to be using a relational database or handling large and complex data to enjoy the benefits of using a relational data model.\nWhen working with relational data, we generally don’t work with tables separately. We will need to join the information from different tables to run our analysis. To join two or more tables we need to learn about keys.\n\n\n4.7.2 Primary and foreign keys\nThe main way in which relational data models encode relationships between different tables is by using keys. Keys are variables whose values uniquely identify observations. For tidy data, where variables and columns are equivalent, a column is a key if it has a different value in each row. This allows us to use keys as unique identifiers that reference particular observations and create links across tables.\nTwo types of keys are common within relational data models:\n\nPrimary Key: chosen key for a table, uniquely identifies each observation in the table,\nForeign Key: reference to a primary key in another table, used to create links between tables.\n\n\n\n4.7.3 Example\nOn our previously normalized data for plants and sites, let’s choose primary keys for these tables and then identify any foreign keys.\nPrimary keys\nFirst, notice that the columns ‘date’, ‘site’ and ‘sp_code’ cannot be primary keys because they have repeated values across rows. The columns ‘sp_height’ and ‘id’ both have different values in each row, so both are candidates for primary keys. However, the decimal values of ‘sp_height’ don’t make it as useful to use it to reference observations. So we chose ‘id’ as the primary key for this table.\nFor the sites table, all three columns could be keys. We chose ‘site’ as the primary key because it is the most succinct and it also allows us to link the sites table with the plants table.\nForeign keys\nThe ‘site’ column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the ‘site’ column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku.\n\n\n\n\n\n\n\n4.7.4 Compound keys\n\n\nIt can also be the case that a variable is not a key, but by combining it with a second variable we get that the combined values uniquely identify the rows. This is called a\n\nCompound Key: a key that is made up of more than one variable.\n\nFor example, the ‘site’ and ‘sp_code’ columns in the plants table cannot be keys on their own, since each has repeated values. However, when we look at their combined values (1-DAPU, 1-DAMA, 2-DAMA, 2-DAPU) we see each row has a unique value. So ‘site’ and ‘sp_code’ together form a compound key.\n\nThere are also other types of keys, like a natural key or a surrogate key. Each type of key has advantages and disadvantages. You can read more about this in  this article.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#joins",
    "href": "session_04.html#joins",
    "title": "Tidy Data",
    "section": "4.8 Joins",
    "text": "4.8 Joins\nFrequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. Regardless of the join we will perform, we need to start by identifying the primary key in each table and how these appear as foreign keys in other tables.\nWhen conceptualizing merges, one can think of two tables, one on the left and one on the right.\n\n\n4.8.1 Inner Join\nAn INNER JOIN is when you merge the subset of rows that have matches in both the left table and the right table.\n\n\n\n4.8.2 Left Join\nA LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table.\n\n\n\n4.8.3 Right Join\nA RIGHT JOIN is the same as a left join, except that all of the rows from the right table are included with matching data from the left, or a missing value. Notice that left and right joins can ultimately be the same depending on the positions of the tables\n\n\n\n4.8.4 Full Outer Join\nFinally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary.\n\nSometimes people represent joins as Venn diagrams, showing which parts of the left and right tables are included in the results for each join. This representation is useful, however, they miss part of the story related to where the missing value comes from in each result.\n\n\n\nImage source: R for Data Science, Wickham & Grolemund.\n\n\nWe suggest reading the Relational Data chapter in the “R for Data Science” book  for more examples and best practices about joins.\n\n\n4.8.5 Entity-Relationship models\nAn Entity-Relationship model (E-R model), also known as an E-R diagram, is a way to draw a compact diagram that reflects the structure and relationships of the tables in a relational database. These can be particularly useful for big databases that have many tables and complex relationships between them.\nWe will explain the steps to drawing a simplified E-R model with our previous plants and sites tables.\nStep 1: Identify the entities in the relational database and add each one in a box. In our case, entities are [plants] and [sites], since we are gathering observations about both of these.\n\n\n\n\n\nStep 2: Add variables for each entity and identify keys. Add the variables as a list inside each box. Then, identify the primary and foreign keys in each of the boxes. To visualize this, we have indicated\n\nthe  primary key  (of each entity) in  red  and\nany  foreign keys  in  blue .\n\n\n\n\n\n\nStep 3: Add relationships between entities.\n\nDraw a line between the boxes of any two entities that have a relationship.\nIdentify which box has the primary key of the other as a foreign key. Let’s call the box that has the foreign key [box1] and the other box [box2]. Using the previous diagram we can see that “site” is the primary key of [sites] and appears as a foreign key in [plants]. So [plants] is [box1] and [sites] is [box2].\nAdd a word describing how [box1] is related to [box2] above the line connecting the two boxes. So, for example, we need to describe how [plants] is related to [sites]. The relation is “a plant is located in a site”, so we write “located” above the line indicating the relationship between [plants] and [sites].\n\n\n\n\n\n\nStep 4: Add cardinality to every relationship in the diagram. At this step we want to quantify how many items in an entity are related to another entity. This is easiest if we reuse the description we found in the previous step. For example, “a plant is located in one site”. Then we add the symbol for “one” at the end of the line going from [plants] to [sites].\n\n\n\n\n\nTo finish, we also indicate how many plants are related to a single site. Since “a site has many plants”, we add the symbol for “many” at the end of the line going from [sites] to [plants]\n\n\n\n\n\nThat’s it!\n\n4.8.5.1 EDR Crow’s Foot\nThe symbols we used at the end of the lines are called ERD “crow’s foot”. You can see all the existing ones together with an example in the next diagram.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you need to produce a publishable E-R model such as the one above,  Mermaid  is a great option. Read more about how to use this tool to create diagrams  here .",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#best-practices-summary",
    "href": "session_04.html#best-practices-summary",
    "title": "Tidy Data",
    "section": "4.9 Best Practices Summary",
    "text": "4.9 Best Practices Summary\nThis is a summary of what we have covered, and some extra advice!\nThe tidy data principles are:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nIn normalized data we organize data so that :\n\nWe have separate tables for each type of entity measured\nObservations (rows) are all unique\nEach column represents either an identifying variable or a measured variable\nEach table follows the tidy data principles\n\nCreating relational data models by assigning primary and foreign keys to each table allows us to maintain relationships between separate normalized tables. Choose the primary key for each table based on your understanding of the data and take efficiency into account. Once you choose a column as the primary key, make sure that all the values in that column are there!\nFor a big relational database, an Entity-Relationship model can be an effective way to explain how different tables and their keys are related to each other. If we need to merge tables we can do it using different types of joins.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#more-on-data-management",
    "href": "session_04.html#more-on-data-management",
    "title": "Tidy Data",
    "section": "4.10 More on Data Management",
    "text": "4.10 More on Data Management\nTidy data is one very important step to data management best practices. However there is more to consider. Here we provide some extra advice from a great paper called  ‘Some Simple Guidelines for Effective Data Management’.\n\nDesign tables to add rows, not columns\nUse a scripted program (like R!)\nNon-proprietary file formats are preferred (eg: csv, txt)\nKeep a raw version of data\nUse descriptive files and variable names (without spaces!)\nInclude a header line in your tabular data files\nUse plain ASCII text\n\nIn the Cleaning & Wrangling chapter we will cover more best practices for cleaning irregular and missing data and how to implement them using R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_04.html#activity",
    "href": "session_04.html#activity",
    "title": "Tidy Data",
    "section": "4.11 Activity",
    "text": "4.11 Activity\n\n\n\n\n\n\n\n\nTidy data, keys and joins\n\n\n\n\nGet together in pairs or small groups\nObtain materials from instructors including activity handout and blank paper(s).\nFollow instructions in activity handout.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "Writing Functions",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "Writing Functions",
    "section": "",
    "text": "Explain the importance of using and developing functions\nCreate custom functions using R code\nDocument functions to improve understanding and code communication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_05.html#r-functions",
    "href": "session_05.html#r-functions",
    "title": "Writing Functions",
    "section": "5.1 R Functions",
    "text": "5.1 R Functions\n\n\n\n\n\n\nQuick reminder: What is a function?\n\n\n\n\n\nA set of statements or expressions of code that are organized together to perform a specific task.\nThe statements or expressions of code within the function accept accept user input(s), does something with it, and returns a useful output.\nSyntax: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...)\n\n\n\nMany people write R code as a single, continuous stream of commands, often drawn from the R Console itself and simply pasted into a script. While any script brings benefits over non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions.\n\n5.1.1 Why Functions?\n\n\n\n\n\n\nDRY: Don’t Repeat Yourself\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).”\nChapter 19 Functions in R for Data Science (Grolemund & Wickham)\n\nBy creating small functions that only complete one logical task and do it well, we quickly gain:\n\nImproved understanding\nReuse via decomposing tasks into bite-sized chunks\nImproved error testing\n\n\n\n\n\n\n\nNaming Functions\n\n\n\nThe name of a function is important. Ideally, function names should be short, but still clearly captures what the function does.\nBest Practices from Chapter 19 Functions in R for Data Science:\n\nFunction names should be verbs and arguments should be nouns (there are exceptions).\nUse the snake_case naming convention for functions that are multiple words.\nFor a “family” of functions, use a common prefix to indicate that they are connected.\n\n\n\n\n\n5.1.2 Exercise: Temperature Conversion\nImagine you have a bunch of data measured in Fahrenheit and you want to convert that for analytical purposes to Celsius. You might have an R script that does this for you.\n\nairtemps &lt;- c(212, 30.3, 78, 32)\ncelsius1 &lt;- (airtemps[1] - 32) * 5/9\ncelsius2 &lt;- (airtemps[2] - 32) * 5/9\ncelsius3 &lt;- (airtemps[3] - 32) * 5/9\n\nNote the duplicated code, where the same formula is repeated three times. This code would be both more compact and more reliable if we didn’t repeat ourselves.\n\nCreate a Function that Converts Fahrenheit to Celsius\nFunctions in R are a mechanism to process some input and return a value. Similarly to other variables, functions can be assigned to a variable so that they can be used throughout code by reference. To create a function in R, you use the function function (so meta!) and assign its result to a variable. Let’s create a function that calculates Celsius temperature outputs from Fahrenheit temperature inputs.\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  return(celsius)\n}\n\nBy running this code, we have created a function and stored it in R’s global environment. The fahr argument to the function function indicates that the function we are creating takes a single parameter (the temperature in Fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before:\n\ncelsius4 &lt;- fahr_to_celsius(airtemps[1])\ncelsius4\n\n[1] 100\n\ncelsius1 == celsius4\n\n[1] TRUE\n\n\nExcellent. So now we have a conversion function we can use. Note that, because most operations in R can take multiple types as inputs, we can also pass the original vector of airtemps, and calculate all of the results at once:\n\ncelsius &lt;- fahr_to_celsius(airtemps)\ncelsius\n\n[1] 100.0000000  -0.9444444  25.5555556   0.0000000\n\n\nThis takes a vector of temperatures in Fahrenheit, and returns a vector of temperatures in Celsius.\n\n\nYour Turn: Create a Function that Converts Celsius to Fahrenheit\n\n\n\n\n\n\nExercise\n\n\n\nCreate a function named celsius_to_fahr that does the reverse, it takes temperature data in Celsius as input, and returns the data converted to Fahrenheit.\nCreate the function celsius_to_fahr in a new R Script file.\nThen use that formula to convert the celsius vector back into a vector of Fahrenheit values, and compare it to the original airtemps vector to ensure that your answers are correct.\nHint: the formula for Celsius to Fahrenheit conversions is celsius * 9/5 + 32.\n\n\nDid you encounter any issues with rounding or precision?\n\n\n\n\n\n\nSolution, but don’t peek!\n\n\n\n\n\nDon’t peek until you write your own…\n\ncelsius_to_fahr &lt;- function(celsius) {\n    fahr &lt;- celsius * 9/5 + 32\n    return(fahr)\n}\n\nresult &lt;- celsius_to_fahr(celsius)\nairtemps == result\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\n\n\n\n\n\n5.1.3 Documenting R Functions\nFunctions need documentation so that we can communicate what they do, and why. The roxygen2 R package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation.\nDocumentation in roxygen2 is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters #'. Here’s a documented version of a function:\n\n#' Convert temperature values from Fahrenheit to Celsius\n#'\n#' @param fahr Numeric or numeric vector in degrees Fahrenheit\n#' \n#' @return Numeric or numeric vector in degrees Celsius\n#' @export\n#' \n#' @examples\n#' fahr_to_celsius(32)\n#' fahr_to_celsius(c(32, 212, 72))\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  return(celsius)\n}\n\nNote the use of the @param keyword to define the expectations of input data, and the @return keyword for defining the value that is returned from the function. The @examples function is useful as a reminder as to how to use the function. Finally, the @export keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize.\n\n\n\n\n\n\nCheck it out: Function Documentation Section from R Packages (2e)\n\n\n\nFor more best practices on function documentation, review Hadley Wickham and Jennifer Bryan’s online book R Packages (2e) - Chapter 10, Section 16: Function Documentation.\n\n\n\n\n5.1.4 Exercise: Minimizing Work with Functions\nFunctions can of course be as simple or complex as needed. They can be be very effective in repeatedly performing calculations, or for bundling a group of commands that are used on many different input data sources. For example, we might create a simple function that takes Fahrenheit temperatures as input, and calculates both Celsius and Kelvin temperatures. All three values are then returned in a list, making it very easy to create a comparison table among the three scales.\n\nconvert_temps &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  kelvin &lt;- celsius + 273.15\n  return(list(fahr = fahr, celsius = celsius, kelvin = kelvin))\n}\n\ntemps_df &lt;- data.frame(convert_temps(seq(-100,100,10)))\n\n\n\n\n\n\n\nOnce we have a dataset like that, we might want to plot it. One thing that we do repeatedly is set a consistent set of display elements for creating graphs and plots. By using a function to create a custom ggplot theme, we can enable to keep key parts of the formatting flexible. For example, in the custom_theme function, we provide a base_size argument that defaults to using a font size of 9 points. Because it has a default set, it can safely be omitted. But if it is provided, then that value is used to set the base font size for the plot.\n\ncustom_theme &lt;- function(base_size = 9) {\n    ggplot2::theme(\n      text             = ggplot2::element_text(family = 'Helvetica', \n                                               color = 'gray30', \n                                               size = base_size),\n      plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25), \n                                               hjust = 0.5, \n                                               face = 'bold'),\n      panel.background = ggplot2::element_blank(),\n      panel.border     = ggplot2::element_blank(),\n      panel.grid.minor = ggplot2::element_blank(),\n      panel.grid.major = ggplot2::element_line(colour = 'grey90', \n                                               linewidth = 0.25),\n      legend.position  = 'right',\n      legend.key       = ggplot2::element_rect(colour = NA, \n                                               fill = NA),\n      axis.ticks       = ggplot2::element_blank(),\n      axis.line        = ggplot2::element_blank()\n      )\n}\n\nlibrary(ggplot2)\n\nggplot(temps_df, mapping = aes(x = fahr, y = celsius, color = kelvin)) +\n    geom_point() +\n    custom_theme(10)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nIn this case, we set the font size to 10, and plotted the air temperatures. The custom_theme function can be used anywhere that one needs to consistently format a plot.\nBut we can go further. One can wrap the entire call to ggplot in a function, enabling one to create many plots of the same type with a consistent structure. For example, we can create a scatterplot function that takes a data frame as input, along with a point_size for the points on the plot, and a font_size for the text.\n\nscatterplot &lt;- function(df, point_size = 2, font_size = 9) {\n  ggplot(df, mapping = aes(x = fahr, y = celsius, color = kelvin)) +\n    geom_point(size = point_size) +\n    custom_theme(font_size)\n}\n\nCalling that let’s us, in a single line of code, create a highly customized plot but maintain flexibility via the arguments passed in to the function. Let’s set the point size to 3 and font to 16 to make the plot more legible.\n\nscatterplot(temps_df, point_size = 3, font_size = 16)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nOnce these functions are set up, all of the plots built with them can be reformatted by changing the settings in just the functions, whether they were used to create 1, 10, or 100 plots.\n\n\n5.1.5 Summary\n\nFunctions are useful to reduce redundancy, reuse code, and reduce errors\nBuild functions with function()\nDocument functions with roxygen2 comments\n\n\n\n\n\n\n\nWorkflow for Creating Functions\n\n\n\n\nHave a clear goal (sometimes it helps to create a visual).\nOutline the plan and then add more detailed steps or tasks.\nBuild it up bit-by-bit and start with a minimum viable example. As your function becomes more complex, it can harder to track all the bits.\nAlways check intermediates!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_06.html",
    "href": "session_06.html",
    "title": "Creating R Packages",
    "section": "",
    "text": "6.1 R Packages\nMost R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases:\nEven if you don’t plan on writing a package with such broad appeal such as, say, ggplot2 or dplyr, you still might consider creating a package to contain:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating R Packages</span>"
    ]
  },
  {
    "objectID": "session_06.html#r-packages",
    "href": "session_06.html#r-packages",
    "title": "Creating R Packages",
    "section": "",
    "text": "Mechanism to redistribute reusable code (even if just for yourself)\nMechanism to reproducibly document analysis and models and their results\n\n\n\nUseful utility functions you write (i.e. a Personal Package). Having a place to put these functions makes it much easier to find and use them later.\nA set of shared routines for your lab or research group, making it easier to remain consistent within your team and also to save time.\nThe analysis accompanying a thesis or manuscript, making it all that much easier for others to reproduce your results.\n\n\n\n\n\n\n\nPackages for Creating and Maintaining Packages\n\n\n\nThe usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience.\n\n\n\n6.1.1 Create a Basic Package\nTo create a package we’re going to use the following packages:\n\ndevtools: Provides R functions that make package development easier by expediting common development tasks.\nusethis: Commonly referred to as a “workflow package” and provides functions that automate common tasks in project setup and development for both R packages and non-package projects.\nroxygen2: Provides a structure for describing your functions in the scripts you’re creating them in. It will additionally process the source code and the documentation within it to automatically create the necessary files for the documentation to appear in your R Package.\n\nThanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is:\n\nusethis::create_package(\"~/mytools\")\n\n✔ Creating '/home/dolinh/mytools/'\n✔ Setting active project to '/home/dolinh/mytools'\n✔ Creating 'R/'\n✔ Writing 'DESCRIPTION'\nPackage: mytools\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last &lt;first.last@example.com&gt; [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n✔ Writing 'NAMESPACE'\n✔ Writing 'mytools.Rproj'\n✔ Adding '^mytools\\\\.Rproj$' to '.Rbuildignore'\n✔ Adding '.Rproj.user' to '.gitignore'\n✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n✔ Opening '/home/dolinh/mytools/' in new RStudio session\n✔ Setting active project to '&lt;no active project&gt;'\n\n\n\n\n\n\nWhat did the create_package() function do?\n\n\n\n\nOpen a new project called mytools (the name of the package) in a new RStduio session.\nCreate a top-level directory structure, including a number of critical files under the standard R package structure:\n\nDESCRIPTIONfile: The most important file, which provides metadata about your package. Edit this file to provide reasonable values for each of the fields, including your contact information.\nNAMESPACE file declares the functions your package exports for external use and the external functions your package imports from other packages.\nR/ directory is where you save all your function scripts and other .R files.\n.Rbuildignore lists files that we need to have around but that should not be included when building the R package from source.\n.Rproj.user is a directory used internally by RStudio.\n\nAdd the Build Tab to the Environment Pane.\n\n\n\n\n\n6.1.2 Add a License\nInformation about choosing a LICENSE is provided in the R Package (2e) book Chapter 12: Licensing.\nThe DESCRIPTION file expects the license to be chose from a predefined list, but you can use its various utility methods for setting a specific license file, such as the MIT license or the Apache 2 license:\n\nusethis::use_apache_license()\n\n✔ Setting License field in DESCRIPTION to 'Apache License (&gt;= 2.0)'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\nOnce your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this:\n\n\n\n\n\n\nPackage: mytools\nTitle: Halina Do-Linh's Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Halina\", \"Do-Linh\", email = \"dolinh@nceas.ucsb.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\n\n\n\n6.1.3 Add Code\nThe skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file custom_theme that contains a function custom_theme() that you might want to reuse. The usethis::use_r() function will help set up you files in the right places. For example, running:\n\nusethis::use_r(\"custom_theme\")\n\n✔ Setting active project to '/home/dolinh/mytools'\n• Modify 'R/custom_theme.R'\n• Call `use_test()` to create a matching test file\ncreates the file R/custom_theme and stores it in the R directory, which you can then modify as needed:\n\ncustom_theme &lt;- function(base_size = 9) {\n    ggplot2::theme(\n      axis.ticks       = ggplot2::element_blank(),\n      text             = ggplot2::element_text(family = 'Helvetica', \n                                               color = 'gray30', \n                                               size = base_size),\n      plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25), \n                                               hjust = 0.5, \n                                               face = 'bold'),\n      panel.background = ggplot2::element_blank(),\n      legend.position  = 'right',\n      panel.border     = ggplot2::element_blank(),\n      panel.grid.minor = ggplot2::element_blank(),\n      panel.grid.major = ggplot2::element_line(colour = 'grey90', \n                                               linewidth = .25),\n      legend.key       = ggplot2::element_rect(colour = NA, \n                                               fill = NA),\n      axis.line        = ggplot2::element_blank()\n      )\n}\n\n\n\n\n\n\n\nPower of Packages\n\n\n\nRemember when we created custom_theme() from the Functions Lesson Section 15.1.4? Now that we’ve added it to our mytools package, we don’t have to worry about coyping the code from another file, sourcing the file from another directory, or copying the script from an R Project.\nInstead we can leverage the portable functionality of a package to easily access our custom functions and maintain the code in one location.\n\n\n\n\n6.1.4 Add Dependencies\nIf your R code depends on functions from another package, you must declare it. In the Imports section in the DESCRIPTION file, list all the packages your functions depend upon.\nIn our custom_theme() function, we depend on the ggplot2 package, and so we need to list it as a dependency.\nOnce again, usethis provides a handy helper method:\n\nusethis::use_package(\"ggplot2\")\n\n✔ Adding 'ggplot2' to Imports field in DESCRIPTION\n• Refer to functions with `ggplot2::fun()`\nTake a look at the DESCRIPTION file again, and you’ll see the Imports section has been added, with ggplot2 underneath.\n\n\n\n\n\n\nPackage: mytools\nTitle: Halina Do-Linh's Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Halina\", \"Do-Linh\", email = \"dolinh@nceas.ucsb.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nImports: \n    ggplot2\n\n\n\n\n\n6.1.5 Add Documentation\nDocumentation is crucial to add to each of your functions. In the Functions Lesson, we did this using the roxygen2 package and that same package and approach can be used for packages.\nThe roxygen2 approach allows us to add comments in the source code, where are then converted into Help pages that we can access by typing ?function_name in the Console.\nLet’s add documentation for the custom_theme() function.\n\n#' My custom ggplot theme\n#'\n#' @param base_size Numeric value of font size of all text elements in plot\n#'\n#' @return A theme used for ggplot point or line plots\n#' @export\n#'\n#' @examples\n#' library(ggplot2)\n#' \n#'   ggplot(data = mtcars, aes(x = mpg, y = disp)) +\n#'     geom_point() +\n#'     custom_theme(base_size = 30)\ncustom_theme &lt;- function(base_size = 9) {\n  ggplot2::theme(\n    axis.ticks       = ggplot2::element_blank(),\n    text             = ggplot2::element_text(family = 'Helvetica',\n                                             color = 'gray30',\n                                             size = base_size),\n    plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25),\n                                             hjust = 0.5,\n                                             face = 'bold'),\n    panel.background = ggplot2::element_blank(),\n    legend.position  = 'right',\n    panel.border     = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_line(colour = 'grey90',\n                                             linewidth = .25),\n    legend.key       = ggplot2::element_rect(colour = NA,\n                                             fill = NA),\n    axis.line        = ggplot2::element_blank()\n  )\n}\n\nOnce your files are documented, you can then process the documentation using devtools::document() to generate the appropriate .Rd files that your package needs. The .Rd files will appear in the man/ directory, which is automatically created by devtools::document().\n\ndevtools::document()\n\nℹ Updating mytools documentation\nℹ Loading mytools\nWriting custom_theme.Rd\nWe now have a package that we can check() and install() and release(). These functions come from the devtools package, but first let’s do some testing.\n\n\n6.1.6 Testing\nYou can test your code using the testthat package’s testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, in the Functions Lesson we created some tests for our fahr_to_celsius functions but ran them line by line in the console.\nFirst, lets add that function to our package. Run the use_r function in the console:\n\nusethis::use_r(\"fahr_to_celsius\")\n\nThen copy the function and documentation into the R script that opens and save the file.\n\n#' Convert temperature values from Fahrenheit to Celsius\n#'\n#' @param fahr Numeric or numeric vector in degrees Fahrenheit\n#' \n#' @return Numeric or numeric vector in degrees Celsius\n#' @export\n#' \n#' @examples\n#' fahr_to_celsius(32)\n#' fahr_to_celsius(c(32, 212, 72))\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr-32)*5/9\n  return(celsius)\n}\n\nNow, set up your package for testing:\n\nusethis::use_testthat()\n\n✔ Setting active project to '/home/dolinh/mytools'\n✔ Adding 'testthat' to Suggests field in DESCRIPTION\n✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n✔ Creating 'tests/testthat/'\n✔ Writing 'tests/testthat.R'\n• Call `use_test()` to initialize a basic test file and open it for editing.\nThen write a test for fahr_to_celsius:\n\nusethis::use_test(\"fahr_to_celsius\")\n\n✔ Writing 'tests/testthat/test-fahr_to_celsius.R'\n• Modify 'tests/testthat/test-fahr_to_celsius.R'\nYou can now add tests to the test-fahr_to_celsius.R, and you can run all of the tests using devtools::test(). For example, if you add a test to the test-fahr_to_celsius.R file:\n\ntest_that(\"fahr_to_celsius works\", {\n  expect_equal(fahr_to_celsius(32), 0)\n  expect_equal(fahr_to_celsius(212), 100)\n})\n\nThen you can run the tests to be sure all of your functions are working using devtools::test():\n\ndevtools::test()\n\nℹ Testing mytools\n✔ | F W S  OK | Context\n✔ |         2 | fahr_to_celsius [0.2s]                                                                                             \n\n══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nDuration: 0.4 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\nYay, all tests passed!\n\n\n6.1.7 Checking and Installing\nNow that you’ve completed testing your package, you can check it for consistency and completeness using devtools::check().\n\ndevtools::check()\n\n══ Documenting ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nℹ Updating mytools documentation\nℹ Loading mytools\n\n══ Building ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• CFLAGS    : -Wall -pedantic -fdiagnostics-color=always\n• CXXFLAGS  : -Wall -pedantic -fdiagnostics-color=always\n• CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX14FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX17FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX20FLAGS: -Wall -pedantic -fdiagnostics-color=always\n── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/dolinh/mytools/DESCRIPTION’ (610ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information (338ms)\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n   \n══ Checking ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• _R_CHECK_CRAN_INCOMING_REMOTE_               : FALSE\n• _R_CHECK_CRAN_INCOMING_                      : FALSE\n• _R_CHECK_FORCE_SUGGESTS_                     : FALSE\n• _R_CHECK_PACKAGES_USED_IGNORE_UNUSED_IMPORTS_: FALSE\n• NOT_CRAN                                     : true\n── R CMD check ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n─  using log directory ‘/tmp/Rtmp1UgqFD/file6d79323df6fae/mytools.Rcheck’ (649ms)\n─  using R version 4.2.2 (2022-10-31)\n─  using platform: x86_64-pc-linux-gnu (64-bit)\n─  using session charset: UTF-8\n─  using options ‘--no-manual --as-cran’\n✔  checking for file ‘mytools/DESCRIPTION’\n─  this is package ‘mytools’ version ‘0.0.0.9000’\n─  package encoding: UTF-8\n✔  checking package namespace information\n✔  checking package dependencies (2.1s)\n✔  checking if this is a source package\n✔  checking if there is a namespace\n✔  checking for executable files\n✔  checking for hidden files and directories\n✔  checking for portable file names\n✔  checking for sufficient/correct file permissions\n✔  checking serialization versions\n✔  checking whether package ‘mytools’ can be installed (3.2s)\n✔  checking installed package size\n✔  checking package directory\n✔  checking for future file timestamps (412ms)\n✔  checking DESCRIPTION meta-information (584ms)\n✔  checking top-level files ...\n✔  checking for left-over files\n✔  checking index information\n✔  checking package subdirectories ...\n✔  checking R files for non-ASCII characters ...\n✔  checking R files for syntax errors ...\n✔  checking whether the package can be loaded (481ms)\n✔  checking whether the package can be loaded with stated dependencies ...\n✔  checking whether the package can be unloaded cleanly ...\n✔  checking whether the namespace can be loaded with stated dependencies ...\n✔  checking whether the namespace can be unloaded cleanly (450ms)\n✔  checking loading without being on the library search path (522ms)\n✔  checking dependencies in R code (1.2s)\n✔  checking S3 generic/method consistency (1s)\n✔  checking replacement functions ...\n✔  checking foreign function calls ...\n✔  checking R code for possible problems (5.2s)\n✔  checking Rd files (449ms)\n✔  checking Rd metadata ...\n✔  checking Rd line widths ...\n✔  checking Rd cross-references ...\n✔  checking for missing documentation entries ...\n✔  checking for code/documentation mismatches (885ms)\n✔  checking Rd \\usage sections (1.3s)\n✔  checking Rd contents ...\n✔  checking for unstated dependencies in examples ...\n✔  checking examples (2.7s)\n✔  checking for unstated dependencies in ‘tests’ ...\n─  checking tests (418ms)\n✔  Running ‘testthat.R’ (1.4s)\n✔  checking for non-standard things in the check directory\n✔  checking for detritus in the temp directory\n   \n   \n── R CMD check results ──────────────────────────────────────────────────────────────────────────────────── mytools 0.0.0.9000 ────\nDuration: 27.3s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\nThen you can install it locally using devtools::install(), which needs to be run from the parent directory of your module\n\ndevtools::install()\n\n── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/dolinh/mytools/DESCRIPTION’ (541ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information ...\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n   \nRunning /opt/R/4.2.2/lib/R/bin/R CMD INSTALL /tmp/Rtmp1UgqFD/mytools_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/dolinh/R/x86_64-pc-linux-gnu-library/4.2’\n* installing *source* package ‘mytools’ ...\n** using staged installation\n** R\n** tests\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (mytools)\nAfter installing, your package is now available for use in your local environment, yay!\n\n\n\n\n\n\nCheck out the Build Tab\n\n\n\nRemember when we ran usethis::create_package() and after we ran it we saw the Build Tab added to the Environment pane?\nIn the Build Tab, each of the buttons correspond with one of the devtools functions we ran, meaning:\n\nTest button is equivalent to running devtools::test() in the Console\nCheck button is equivalent to running devtools::check() in the Console\nInstall button is equivalent to running devtools::install() in the Console\n\n\n\n\n\n6.1.8 Sharing and Releasing\n\nGitHub: The simplest way to share your package with others is to upload it to a GitHub repository, which allows others to install your package using the install_github('mytools','github_username') function from devtools.\nCRAN: If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from devtools(). Releasing a package to CRAN requires a significant amount of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process.\nR-Universe: A newer approach is to link your package release to R-Universe, which is an effective way to make it easy to test and maintain packages so that many people can install them using the familiar install.pacakges() function in R. In R-Universe, people and organizations can create their own universe of packages, which represent a collection of packages that appear as a CRAN-compatible repository in R. For example, for DataONE we maintain the DataONE R-Universe, which lists the packages we actively maintain as an organization. So, any R-user that wants to install these packages can do so by adding our universe to their list of repositories, and then installing packages as normal. For example, to install the codyn package, one could use:\n\n\ninstall.packages('codyn', repos = c('https://dataoneorg.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n6.1.9 Exercise: Add More Functions\nAdd additional temperature conversion functions to the mytools package and:\n\nAdd full documentation for each function\nWrite tests to ensure the functions work properly\nRebuild the package using document(), check(), and install()\n\n\n\n\n\n\n\nDon’t forget to update the version number before you install!\n\n\n\nVersion information is located in the DESCRIPTION file and when you first create a package the version is 0.0.0.9000.\nThis version number follows the format major.minor.patch.dev. The different parts of the version represent different things:\n\nMajor: A significant change to the package that would be expected to break users code. This is updated very rarely when the package has been redesigned in some way.\nMinor: A minor version update means that new functionality has been added to the package. It might be new functions to improvements to existing functions that are compatible with most existing code.\nPatch: Patch updates are bug fixes. They solve existing issues but don’t do anything new.\nDev: Dev versions are used during development and this part is missing from release versions. For example you might use a dev version when you give someone a beta version to test. A package with a dev version can be expected to change rapidly or have undiscovered issues.\n\nAfter you’ve made some changes to a package, but before you install run the code:\n\nusethis::use_version()\n\nCurrent version is 0.0.0.9000.\nWhat should the new version be? (0 to exit) \n\n1: major --&gt; 1.0.0\n2: minor --&gt; 0.1.0\n3: patch --&gt; 0.0.1\n4:   dev --&gt; 0.0.0.9001\nSince we’re adding new functions, we can consider this a minor change and can select option 2.\nSelection: 2\n✔ Setting Version field in DESCRIPTION to '0.1.0'\nSource: COMBINE’s R package workshop, Ch 9: Versioning\n\n\n\n\n6.1.10 Additional Resources\n\nHadley Wickham and Jenny Bryan’s awesome book: R Packages\nROpenSci Blog Post: How to create your personal CRAN-like repository on R-universe\nKarl Broman’s: R package primer: a minimal tutorial on writing R packages\nThomas Westlake’s Short Tutorial: Writing an R package from scratch (his post is an updated version of Hilary Parker’s blog post)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating R Packages</span>"
    ]
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "Practice Session: Joins",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#learning-objectives",
    "href": "session_07.html#learning-objectives",
    "title": "Practice Session: Joins",
    "section": "",
    "text": "Practice joining tables together\nPractice identifying primary and foreign keys\nPractice using common cleaning and wrangling functions\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThese exercises are adapted from Allison Horst’s EDS 221: Scientific Programming Essentials Course for the Bren School’s Master of Environmental Data Science program.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#about-the-data",
    "href": "session_07.html#about-the-data",
    "title": "Practice Session: Joins",
    "section": "About the data",
    "text": "About the data\nThese exercises will be using bird survey data collected from the central Arizona-Phoenix metropolitan area by Arizona State University researchers (Warren et al. 2021).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#exercise-1-practice-joins",
    "href": "session_07.html#exercise-1-practice-joins",
    "title": "Practice Session: Joins",
    "section": "Exercise 1: Practice Joins",
    "text": "Exercise 1: Practice Joins\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes in the remote repository (aka repository on GitHub).\nCreate a new Quarto Document.\n\nTitle it “R Practice: Tidy Data and Joins”.\nSave the file and name it “r-practice-tidy-data-joins” in your scripts folder.\n\n\nNote: Double check that you’re in the right project. Where in RStudio can you check where you are?\n\nLoad the following libraries at the top of your Quarto Document.\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(here)\n\nhere() starts at C:/Users/casey/Documents/github/2025-05-coreR\n\nlibrary(lubridate) # for bonus question\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n# Quick question: Do you get a message after loading the libraries? What is it telling you? Talk to your neighbor about it or write a note in your qmd.\n\n\nObtain data from the EDI Data Portal Ecological and social interactions in urban parks: bird surveys in local parks in the central Arizona-Phoenix metropolitan area. Download the following files:\n\n\n52_pp52_birds_1.csv\n52_pp52_surveys_1.csv\n52_pp52_sites_1.csv\n52_pp52_taxalist_1.csv\n\nNote: It’s up to you on how you want to download and load the data! You can either use the download links (obtain by right-clicking the “Download” button and select “Copy Link Address” for each data entity) or manually download the data and then upload the files to RStudio server.\n\nOrganize your Quarto Document in a meaningful way. Organization is personal - so this is up to you! Consider the different ways we’ve organized previous files using: headers, bold text, naming code chunks, comments in code chunks. What is most important is organizing and documenting the file so that your future self (or if you share this file with others!) understands it as well as your current self does right now.\nUse the Git workflow. After you’ve set up your project and uploaded your data go through the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\nNote: You also want to Pull when you first open a project.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#read-in-the-data",
    "href": "session_07.html#read-in-the-data",
    "title": "Practice Session: Joins",
    "section": "7.1 Read in the data",
    "text": "7.1 Read in the data\n\n\n\n\n\n\nQuestion 1\n\n\n\nRead in the data and store the data frames as bird_observations, sites, surveys, and taxalist (it should be clear from the raw file names which is which).\n\n\n\n# read in data using download links\nbird_observations &lt;- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=53edaa7a0e083013d9bf20322db1780e\")\n\nRows: 40425 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): site_id, species_id, distance, notes, direction\ndbl (4): survey_id, bird_count, seen, heard\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsurveys &lt;- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=b2466fa5cb5ed7ee1ea91398fc291c59\")\n\nRows: 2004 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site_id, observer, wind_dir, notes\ndbl  (4): survey_id, wind_speed, air_temp, cloud_cover\nlgl  (1): temp_units\ndttm (3): survey_date, time_start, time_end\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsites &lt;- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=81bf72420e69077097fb0790dcdc63a6\")\n\nRows: 221 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): site_id, park_code, park_district, park_name, point_code\nlgl (2): point_location, park_acreage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntaxalist &lt;- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=58f863b7e3066e68536a9cacdc7bd58e\")\n\nRows: 259 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): species_id, common_name\ndbl (1): asu_itis\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# read in data from the data directory after manually downloading data \nbird_observations &lt;- read_csv(here::here(\"data/52_pp52_birds_1.csv\"))\nsurveys &lt;- read_csv(here::here(\"data/52_pp52_surveys_1.csv\"))\nsites &lt;- read_csv(here::here(\"data/52_pp52_sites_1.csv\"))\ntaxalist &lt;- read_csv(here::here(\"data/52_pp52_taxalist_1.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#get-familiar-with-the-data",
    "href": "session_07.html#get-familiar-with-the-data",
    "title": "Practice Session: Joins",
    "section": "7.2 Get familiar with the data",
    "text": "7.2 Get familiar with the data\n\n\n\n\n\n\nQuestion 2a\n\n\n\nWhat functions can you use to explore the data you just read in? Think about which functions we’ve been using to explore the structure of the data frame, information about columns, unique observations, etc. Tip: run View(name_of_your_data_frame) in the console to see data in a spreadsheet-style viewer.\n\n\n\n# returns dimensions of the dataframe by number of rows and number of cols\ndim(bird_observations)\n\n[1] 40425     9\n\n# returns the top six rows of the dataframe\nhead(bird_observations)\n\n# A tibble: 6 × 9\n  survey_id site_id species_id distance bird_count notes  seen heard direction\n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1       144 LI-S    HOSP       5-10              4 &lt;NA&gt;      1     1 NE       \n2       145 LI-W    HOSP       20-40            10 &lt;NA&gt;      0     1 E        \n3       145 LI-W    AUWA       20-40             2 &lt;NA&gt;      0     1 SE       \n4       145 LI-W    RODO       FT                2 &lt;NA&gt;      1     0 E        \n5       145 LI-W    GTGR       &gt;40               2 &lt;NA&gt;      0     1 NE       \n6       145 LI-W    WCSP       20-40             3 &lt;NA&gt;      0     1 N        \n\n# returns all the columns and some info about the cols\nglimpse(bird_observations)\n\nRows: 40,425\nColumns: 9\n$ survey_id  &lt;dbl&gt; 144, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145,…\n$ site_id    &lt;chr&gt; \"LI-S\", \"LI-W\", \"LI-W\", \"LI-W\", \"LI-W\", \"LI-W\", \"LI-W\", \"LI…\n$ species_id &lt;chr&gt; \"HOSP\", \"HOSP\", \"AUWA\", \"RODO\", \"GTGR\", \"WCSP\", \"WCSP\", \"GT…\n$ distance   &lt;chr&gt; \"5-10\", \"20-40\", \"20-40\", \"FT\", \"&gt;40\", \"20-40\", \"20-40\", \"F…\n$ bird_count &lt;dbl&gt; 4, 10, 2, 2, 2, 3, 3, 2, 2, 3, 1, 10, 3, 1, 6, 6, 20, 12, 2…\n$ notes      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ seen       &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,…\n$ heard      &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,…\n$ direction  &lt;chr&gt; \"NE\", \"E\", \"SE\", \"E\", \"NE\", \"N\", \"E\", \"E\", \"S\", \"E\", \"NE\", …\n\n# similar to glimpse but returns some summary statistics about the cols\nsummary(bird_observations)\n\n   survey_id      site_id           species_id          distance        \n Min.   :   1   Length:40425       Length:40425       Length:40425      \n 1st Qu.: 570   Class :character   Class :character   Class :character  \n Median :1028   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1043                                                           \n 3rd Qu.:1550                                                           \n Max.   :2001                                                           \n                                                                        \n   bird_count          notes                seen            heard       \n Min.   :   1.000   Length:40425       Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:   1.000   Class :character   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :   2.000   Mode  :character   Median :1.0000   Median :0.0000  \n Mean   :   2.938                      Mean   :0.8463   Mean   :0.4967  \n 3rd Qu.:   3.000                      3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1000.000                      Max.   :1.0000   Max.   :1.0000  \n NA's   :33                                                             \n  direction        \n Length:40425      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n# returns column names \nnames(bird_observations)\n\n[1] \"survey_id\"  \"site_id\"    \"species_id\" \"distance\"   \"bird_count\"\n[6] \"notes\"      \"seen\"       \"heard\"      \"direction\" \n\n# returns unique values in a column. In this case we can see all the different bird species IDs\nunique(bird_observations$species_id)\n\n  [1] \"HOSP\" \"AUWA\" \"RODO\" \"GTGR\" \"WCSP\" \"MODO\" \"NOMO\" \"EUST\" \"ANHU\" \"CBTH\"\n [11] \"INDO\" \"GIWO\" \"HOFI\" \"VERD\" \"GHJU\" \"ORJU\" \"KILL\" \"BCHU\" \"ABTO\" \"CACW\"\n [21] \"WEME\" \"CORA\" \"NOHA\" \"NOFL\" \"PYRR\" \"CANW\" \"GIFL\" \"SCJU\" \"BRCR\" \"RCKI\"\n [31] \"COHU\" \"BGGN\" \"SAPH\" \"AMKE\" \"HAHA\" \"HOLA\" \"LOSH\" \"AMGO\" \"BEVI\" \"OCWA\"\n [41] \"BRSP\" \"COYE\" \"SPTO\" \"WBNU\" \"noca\" \"BTSP\" \"ROWR\" \"PHAI\" \"RWBL\" \"FEHA\"\n [51] \"COHA\" \"RTHA\" \"ATFL\" \"BHCO\" \"BRBL\" \"UNDO\" \"SOVI\" \"MALL\" \"SSHA\" \"CEDW\"\n [61] \"AMRO\" \"WEBL\" \"GAQU\" \"LEWO\" \"unwa\" \"CAGO\" \"LEGO\" \"broc\" \"WWDO\" \"COFL\"\n [71] \"SOSP\" \"NRWS\" \"GBHE\" \"WEKI\" \"NAWA\" \"LBWO\" \"BTGN\" \"YWAR\" \"UDEJ\" \"BCNH\"\n [81] \"BTYW\" \"LUWA\" \"CLSW\" \"CAKI\" \"UNTA\" \"PFLB\" \"UNHA\" \"HRSH\" \"BETH\" \"UNWO\"\n [91] \"RSFL\" \"WIWA\" \"WEFL\" \"unhu\" \"GRRO\" \"ECDO\" \"BROC\" \"HOOR\" \"NOCA\" \"TOWA\"\n[101] \"YHBL\" \"WTSW\" \"UNTH\" \"RUHU\" \"WETA\" \"AMCO\" \"LENI\" \"UYRW\" \"LASP\" \"RNSA\"\n[111] \"UNFL\" \"BLPH\" \"MGWA\" \"TUVU\" \"UNBL\" \"TRES\" \"UNSP\" \"GREG\" \"SNEG\" \"UNSW\"\n[121] \"CHSP\" \"WAVI\"\n\n\n\n\n\n\n\n\nQuestion 2b\n\n\n\nWhat are the primary and foreign keys for the tables bird_observations and taxalist? Recall that a primary key is a unique identifier for each observed entity, one per row. And a foreign key references to a primary key in another table (linkage).\nHint: First identify the primary keys for all the tables, then identify the foreign keys.\n\n\n\n\nAnswer\n\n\nbird_observations: Primary key is a compound key made up of survey_id, site_id, and species_id. The foreign key is species_id.\ntaxalist: Primary key is species_id and does not have a foreign key that match the primary key in bird_observations.\n\nHowever, we could join bird_observations and taxalist by species_id, but depending on the type of join some values would be droped or NAs would be introduce in the resulting data frame.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#create-a-subset-of-bird_observations",
    "href": "session_07.html#create-a-subset-of-bird_observations",
    "title": "Practice Session: Joins",
    "section": "7.3 Create a subset of bird_observations",
    "text": "7.3 Create a subset of bird_observations\n\n\n\n\n\n\nQuestion 3\n\n\n\nWrite code to create a subset of bird_observations called birds_subset that only contains observations for birds with species id BHCO and RWBL, and from sites with site ID LI-W and NU-C.\nHint: What function do you use to subset data by rows?\n\n\n\nbirds_subset &lt;- bird_observations %&gt;% \n  filter(species_id %in% c(\"BHCO\", \"RWBL\")) %&gt;% \n  filter(site_id %in% c(\"LI-W\", \"NU-C\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#use-left_join-to-merge-birds_subset-with-the-tables-sites",
    "href": "session_07.html#use-left_join-to-merge-birds_subset-with-the-tables-sites",
    "title": "Practice Session: Joins",
    "section": "7.4 Use left_join() to merge birds_subset with the tables sites",
    "text": "7.4 Use left_join() to merge birds_subset with the tables sites\n\n\n\n\n\n\nQuestion 4a\n\n\n\nFirst, answer: what do you expect the outcome data frame when doing left_join() between birds_subset and sites to look like? What observations do you expect in the outcome data frame.\nYou can use paper to draw if that helps you or talk to your neighbor. Write down the steps and expected outcome in your Quarto Document.\n\n\n\n\nAnswer\n\nI expect to see all columns and all observations from birds_subset and from sites, I expect to see the columns park_code, park_district, park-name, point_code, point_location and park_acreage and only observations for NU-C and LI-W because those are the only site_id values in birds_subset and in a left join only the observations matching the left table (in this case, birds_subset is the left table) will be kept.\n\n\n\n\n\n\n\nQustion 4b\n\n\n\nUse a left join to update birds_subset so that it also includes sites information. For each join, include an explicit argument saying which key you are joining by (even if it will just assume the correct one for you). Store the updated data frame as birds_left. Make sure to look at the output - is what it contains consistent with what you expected it to contain?\n\n\n\n# syntax using pipe\nbirds_left &lt;- birds_subset %&gt;% \n  left_join(y = sites, by = \"site_id\")\n# don't see x = birds_subset here because piping in birds_subset means it automatically assumes birds_subset as x.\n\n# syntax without pipe\nbirds_left &lt;- left_join(x = birds_subset, y = sites, by = \"site_id\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#use-full_join-to-merge-birds_subset-and-sites-tables",
    "href": "session_07.html#use-full_join-to-merge-birds_subset-and-sites-tables",
    "title": "Practice Session: Joins",
    "section": "7.5 Use full_join() to merge birds_subset and sites tables",
    "text": "7.5 Use full_join() to merge birds_subset and sites tables\n\n\n\n\n\n\nQuestion 5a\n\n\n\nFirst, answer: what do you expect a full_join() between birds_subset and sites to contain? Write this in your Quarto Document or tell a neighbor.\n\n\n\n\nAnswer\n\nI expect to see all columns and all observations from birds_subset and all columns and all observations from sites to be merged into one data frame because in a full join everything is kept. NA values could be introduced.\n\n\n\n\n\n\n\nQuestions 5b\n\n\n\nWrite code to full_join() the birds_subset and sites data into a new object called birds_full. Explicitly include the variable you’re joining by. Look at the output. Is it what you expected?\n\n\n\n# syntax using pipe\nbirds_full &lt;- birds_subset %&gt;% \n  full_join(y = sites, by = \"site_id\")\n\n# syntax without pipe\nbirds_full &lt;- full_join(x = birds_subset, y = sites, by = \"site_id\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#use-inner_join-to-merge-birds_subset-and-taxalist-data",
    "href": "session_07.html#use-inner_join-to-merge-birds_subset-and-taxalist-data",
    "title": "Practice Session: Joins",
    "section": "7.6 Use inner_join() to merge birds_subset and taxalist data",
    "text": "7.6 Use inner_join() to merge birds_subset and taxalist data\n\n\n\n\n\n\nQuestion 6a\n\n\n\nFirst, answer: what do you expect an inner_join() between birds_subset and taxalist to contain? Write this in your Quarto Document or tell a neighbor.\n\n\n\n\nAnswer\n\nI expect to only have data merge together based on species_id and since there is only BHCO and RWBL in birds_subset then I will only retain data related to those two species. I will also expect to see the columns from taxalist: common_name and asu_itis to be merged into the joined table.\n\n\n\n\n\n\n\nQuestion 6b\n\n\n\nWrite code to inner_join() the birds_subset and taxalist, called birds_inner. Include an argument for what variable you’ll be joining by. Make sure you check the output.\n\n\n\n# syntax using pipe\nbirds_inner &lt;- birds_subset %&gt;% \n  inner_join(y = taxalist, by = \"species_id\")\n\n# syntax without pipe\nbirds_inner &lt;- inner_join(x = birds_subset, y = taxalist, by = \"species_id\" )\n\n\n\n\n\n\n\nQuestion 6c\n\n\n\nWhat would you get if instead of inner_join() you’d used left_join() for this example? Write code for the left join and check.\n\n\n\n# syntax using pipe\nbirds_inner_left &lt;- birds_subset %&gt;% \n  left_join(y = taxalist, by = \"species_id\")\n\n# syntax without pipe\nbirds_inner_left &lt;- left_join(x = birds_subset, y = taxalist, by = \"species_id\")\n\n\n\n\n\n\n\nQuestion 6d\n\n\n\nWhy does that make sense for this scenario? In what case would you expect the outcome to differ from an inner_join()? Write this in your Quarto Document or tell a neighbor.\n\n\n\n\nAnswer\n\nYou have the same resulting data set regardless of using inner_join() or left_join() to merge bird_subset and taxalist. The reasons for this are:\n\ninner_join() keeps only the rows (observations) that have a matching key across both data sets - here, species_id is our key, and the only rows that match across both data sets are those where species_id equals BHCO or RWBL\nleft_join() keeps all rows from the left table (in our case, the left table is birds_subset) and merges on data with matching keys (species_id) on the right (here, the right table is taxalist). Because our left data set (birds_subset) only contains species_ids equal to BHCO or RWBL, only rows with those species will be kept from the right data set (taxalist)\n\nYou’d expect the outcome to differ from an inner_join() if birds_subset contained an observation with a species_id that was not found in taxalist. If there was an observation of a species_id in birds_subset that was not in taxalist, then that observation would be kept, and NAs would be assigned to the common_name and asu_itis columns for that observations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#exercise-2-practice-wrangling-joining-data",
    "href": "session_07.html#exercise-2-practice-wrangling-joining-data",
    "title": "Practice Session: Joins",
    "section": "Exercise 2: Practice Wrangling & Joining Data",
    "text": "Exercise 2: Practice Wrangling & Joining Data",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#wrangle-bird_observations-data-and-merge-the-data-with-all-the-other-tables-sites-surveys-and-taxalist",
    "href": "session_07.html#wrangle-bird_observations-data-and-merge-the-data-with-all-the-other-tables-sites-surveys-and-taxalist",
    "title": "Practice Session: Joins",
    "section": "7.7 Wrangle bird_observations data and merge the data with all the other tables (sites, surveys, and taxalist)",
    "text": "7.7 Wrangle bird_observations data and merge the data with all the other tables (sites, surveys, and taxalist)\n\n\n\n\n\n\nQuestion 7a\n\n\n\nStarting with your object bird_observations, rename the notes column to bird_obs_notes (so this doesn’t conflict with notes in the surveys table).\n\n\n\nbird_observations &lt;- bird_observations %&gt;% \n  rename(bird_obs_notes = notes)\n\n\n\n\n\n\n\nQuestion 7b\n\n\n\n\nCreate a subset that contains all observations in the birds_observations data frame,\nthen join the taxalist, sites and surveys tables to it,\nand finally limit to only columns survey_date, common_name, park_name, bird_count, and observer.\n\nHint: What function do you use to subset data by columns?\n\n\n\nbird_obs_subset &lt;- bird_observations %&gt;% \n  full_join(y = taxalist, by = \"species_id\") %&gt;% \n  full_join(y = sites, by = \"site_id\") %&gt;% \n  full_join(y = surveys, by = c(\"site_id\", \"survey_id\")) %&gt;%  \n  select(survey_date, common_name, park_name, bird_count, observer)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#explore-observer-data-and-fix-the-values-within-this-column-so-that-all-values-are-in-the-same-format",
    "href": "session_07.html#explore-observer-data-and-fix-the-values-within-this-column-so-that-all-values-are-in-the-same-format",
    "title": "Practice Session: Joins",
    "section": "7.8 Explore observer data and fix the values within this column so that all values are in the same format",
    "text": "7.8 Explore observer data and fix the values within this column so that all values are in the same format\n\n\n\n\n\n\nQuestion 8a\n\n\n\nContinuing with bird_obs_subset, first use unique() to see the different unique values in the column observer. How many observers are there? Which value is unlike the others?\n\n\n\nunique(bird_obs_subset$observer)\n\n[1] \"B. Rambo\"   \"J. Lemmer\"  \"D. Stuart\"  \"C. Putnam\"  \"S. Lerman\" \n[6] \"Josh Burns\" NA          \n\n\n\n\n\n\n\n\nQuestion 8b\n\n\n\nReplace “Josh Burns” with a format that matches the other observer names. Then use unique() again to check your work.\nHint: What function do you use when you are making a change to an entire column?\n\n\n\nbird_obs_subset &lt;- bird_obs_subset %&gt;% \n  mutate(observer = if_else(condition = observer == \"Josh Burns\", \n                            true = \"J. Burns\", \n                            false = observer))\n\nunique(bird_obs_subset$observer)\n\n[1] \"B. Rambo\"  \"J. Lemmer\" \"D. Stuart\" \"C. Putnam\" \"S. Lerman\" \"J. Burns\" \n[7] NA         \n\n\n\n\n\n\n\n\nSave your work and dont’s forget the Git and GitHub Workflow\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_07.html#bonus-use-a-new-package-lubridate-to-wrangle-the-date-data-and-find-the-total-number-of-birds-by-park-and-month",
    "href": "session_07.html#bonus-use-a-new-package-lubridate-to-wrangle-the-date-data-and-find-the-total-number-of-birds-by-park-and-month",
    "title": "Practice Session: Joins",
    "section": "7.9 Bonus: Use a new package lubridate to wrangle the date data and find the total number of birds by park and month",
    "text": "7.9 Bonus: Use a new package lubridate to wrangle the date data and find the total number of birds by park and month\nHint: How do you learn about a new function or package?\n\n\n\n\n\n\nBonus Question(s)\n\n\n\n\nUse lubridate::month() to add a new column to bird_obs_subset called survey_month, containing only the month number. Then, convert the month number to a factor (again within mutate()).\nUse dplyr::relocate() to move the new survey_month column to immediately after the survey_date column. You can do this in a separate code chunk, or pipe straight into it from your existing code.\nFilter to only include parks Lindo, Orme, Palomino, and Sonrisa.\nFind the total number of birds observed by park and month (Hint: You can use group_by() and summarize()).\n\n\n\n\nbird_obs_subset &lt;- bird_obs_subset %&gt;% \n  mutate(survey_month = lubridate::month(survey_date)) %&gt;% \n  mutate(survey_month = as.factor(survey_month)) %&gt;% \n  dplyr::relocate(survey_month, .after = survey_date) %&gt;% \n  filter(park_name %in% c(\"Lindo\", \"Orme\", \"Palomino\", \"Sonrisa\")) %&gt;% \n  group_by(park_name, survey_month) %&gt;% \n  summarize(tot_bird_count_month = n())\n\n`summarise()` has grouped output by 'park_name'. You can override using the\n`.groups` argument.\n\n\nTake a look at your final data frame. Does it give you the outcome you expected? Is it informative? How would you improve this wrangling process?\n\n\n\n\nWarren, Paige S., Ann Kinzig, Chris A Martin, and Louis Machabee. 2021. “Ecological and Social Interactions in Urban Parks: Bird Surveys in Local Parks in the Central Arizona-Phoenix Metropolitan Area.” Environmental Data Initiative. https://doi.org/10.6073/PASTA/F6F004BC7112CE266FDE2B80FAD19FF4.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Practice Session: Joins</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "Working in R & RStudio",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "Working in R & RStudio",
    "section": "",
    "text": "Get oriented with the RStudio interface\nRun code and basic arithmetic in the Console\nPractice writing code in an R Script\nBe introduced to built-in R functions\nUse the Help pages to look up function documentation\n\n\nThis lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. We highly recommend reading through the original lessons and using them as reference (see in the resources section below).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#welcome-to-r-programming",
    "href": "session_08.html#welcome-to-r-programming",
    "title": "Working in R & RStudio",
    "section": "8.1 Welcome to R Programming",
    "text": "8.1 Welcome to R Programming\n\n\n\nArtwork by Allison Horst\n\n\nThere is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source!\nSo what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task.\nAnd if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. Although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together.\nSomething else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunication that can be frustrating, but hands down we are all better off because of it.\nWhile language is a familiar concept, programming languages are in a different context from spoken languages and you will understand this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast.” So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: “desayuno”). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you to increase your expectations and learn to ask and find what you’re looking for.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#rstudio-ide",
    "href": "session_08.html#rstudio-ide",
    "title": "Working in R & RStudio",
    "section": "8.2 RStudio IDE",
    "text": "8.2 RStudio IDE\nLet’s take a tour of the RStudio interface.\n\nNotice the default panes:\n\nConsole (entire left)\nEnvironment/History (tabbed in upper right)\nFiles/Plots/Packages/Help (tabbed in lower right)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can change the default location of the panes, among many other things, see Customizing RStudio.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#coding-in-the-console",
    "href": "session_08.html#coding-in-the-console",
    "title": "Working in R & RStudio",
    "section": "8.3 Coding in the Console",
    "text": "8.3 Coding in the Console\n\n\n\n\n\n\nBut first, an important first question: where are we?\n\n\n\nIf you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/.\n\n\n\n\n\n\n\nWe can run code in a couple of places in RStudio, including the Console, let’s start there.\nAt it’s most basic, we can use R as a calculator, let’s try a couple of examples in the console.\n\n# run in the console\n# really basic examples\n3*4\n3+4\n3-4\n3/4\n\nWhile there are many cases where it makes sense to type code directly in to the the console, it is not a great place to write most of your code since you can’t save what you ran. A better way is to create an R Script, and write your code there. Then when you run your code from the script, you can save it when you are done. We’re going to continue writing code in the Console for now, but we’ll code in an R Script later in this lesson\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you’re in the console you’ll see a greater than sign (&gt;) at the start of a line. This is called the “prompt” and when we see it, it means R is ready to accept commands. If you see a plus sign (+) in the Console, it means R is waiting on additional information before running. You can always press escape (esc) to return to the prompt. Try practicing this by running 3* (or any incomplete expression) in the console.\n\n\n\n8.3.1 Objects in R\nLet’s say the value of 12 that we got from running 3*4 is a really important value we need to keep. To keep information in R, we need to create an object. The way information is stored in R is through objects.\nWe can assign a value of a mathematical operation (and more!) to an object in R using the assignment operator, &lt;- (greater than sign and minus sign). All objects in R are created using the assignment operator, following this form: object_name &lt;- value.\n\n\n\n\n\n\nExercise: Create an object\n\n\n\nAssign 3*4 to an object called important_value and then inspect the object you just created.\n\n\n\n# think of this code as someone saying \"important_value gets 12\".\nimportant_value &lt;- 3*4\n\nNotice how after creating the object, R doesn’t print anything. However, we know our code worked because we see the object, and the value we wanted to store is now visible in our Global Environment. We can force R to print the value of the object by calling the object name (aka typing it out) or by using parentheses.\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you begin typing an object name RStudio will automatically show suggested completions for you that you can select by hitting tab, then press return.\n\n\n\n# printing the object by calling the object name\nimportant_value\n\n[1] 12\n\n# printing the object by wrapping the assignment syntax in parentheses\n(important_value &lt;- 3*4)\n\n[1] 12\n\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you’re in the Console use the up and down arrow keys to call your command history, with the most recent commands being shown first.\n\n\n\n\n8.3.2 Naming Conventions\nBefore we run more calculations, let’s talk about naming objects. For the object, important_value we used an underscore to separate the object name. This naming convention is called snake case. There are other naming conventions including, but not limited to:\n\nwe_used_snake_case\nsomeUseCamelCase\nSomeUseUpperCamelCaseAlsoCalledPascalCase\n\nChoosing a naming convention is a personal preference, but once you choose one - be consistent! A consistent naming convention will increase the readability of your code for others and your future self.\n\n\n\n\n\n\nQuick Tip\n\n\n\nObject names cannot start with a digit and cannot contain certain characters such as a comma or a space.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#running-code-in-an-r-script",
    "href": "session_08.html#running-code-in-an-r-script",
    "title": "Working in R & RStudio",
    "section": "8.4 Running code in an R Script",
    "text": "8.4 Running code in an R Script\nSo far we’ve been running code in the Console, let’s try running code in an R Script. An R Script is a simple text file. RStudio uses an R Script by copying R commands from text in the file and pastes them into the Console as if you were manually entering commands yourself.\n\n\n\n\n\n\nCreating an R Script\n\n\n\n\nFrom the “File” menu, select “New File”\nClick “R Script” from the list of options\n\nRStudio should open your R Script automatically after creating it. Notice a new pane appears above the Console. This is called the Source pane and is where we write and edit R code and documents. This pane is only present if there are files open in the editor.\n\nSave the R Script in your script folder, name the file intro-to-programming.R\n\n\n\n\n8.4.1 How to run code in an R Script\nRunning code in an R Script is different than running code in the Console (aka you can’t just press return / enter). To interpret and run the code you’ve written, R needs you to send the code from the script (or editor) to the Console. Some common ways to run code in an R Script include:\n\nPlace your cursor on the line of code you want to run and use the shortcut command + return or click the Run button in the top right of the Source pane.\nHighlight the code you want to run, then use the shortcut command + return or click the Run button.\n\n\n\n8.4.2 R calculations with objects\nSo we know that objects are how R stores information, and we know we create objects using the assignment operator &lt;-. Let’s build upon that and learn how to use an object in calculations.\nImagine we have the weight of a dog in kilograms. Create the object weight_kg and assign it a value of 25.\n\n# weight of a dog in kilograms\nweight_kg &lt;- 25\n\nNow that R has weight_kg saved in the Global Environment, we can run calculations with it.\n\n\n\n\n\n\nExercise: Using weight_kg run a simple calculation\n\n\n\nLet’s convert the weight into pounds. Weight in pounds is 2.2 times the weight in kg.\n\n\n\n# converting weight from kilograms to pounds\n2.2 * weight_kg\n\n[1] 55\n\n\nYou can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is with the function c(), which stands for combine or concatenate.\nFirst let’s create a vector of weights in kilograms using c() (we’ll talk more about vectors in the next section, Data structures in R).\n\n# create a vector of weights in kilograms\nweight_kg &lt;- c(25, 33, 12)\n# call the object to inspect\nweight_kg\n\n[1] 25 33 12\n\n\nNow convert the vector weight_kg to pounds.\n\n# covert `weight_kg` to pounds \nweight_kg * 2.2\n\n[1] 55.0 72.6 26.4\n\n\nWouldn’t it be helpful if we could save these new weight values we just converted? This might be important information we may need for a future calculation. How would you save these new weights in pounds?\n\n# create a new object \nweight_lb &lt;- weight_kg * 2.2\n# call `weight_lb` to check if the information you expect is there\nweight_lb\n\n[1] 55.0 72.6 26.4\n\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou will make many objects and the assignment operator &lt;- can be tedious to type over and over. Instead, use RStudio’s keyboard shortcut: option + - (the minus sign).\nNotice that RStudio automatically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces.\nRStudio offers many handy keyboard shortcuts. Also, option+Shift+K brings up a keyboard shortcut reference card.\nFor more RStudio tips, check out Master of Environmental Data Science (MEDS) workshop: IDE Tips & Tricks.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#data-types-class-in-r",
    "href": "session_08.html#data-types-class-in-r",
    "title": "Working in R & RStudio",
    "section": "8.5 Data types (class) in R",
    "text": "8.5 Data types (class) in R\n\nCommon data types in R\n\n\n\n\n\n\nData Type\nDefinition\n\n\n\n\nboolean (also called logical)\nData take on the value of either TRUE, FALSE, or NA. NA is used to represent missing values.\n\n\ncharacter\nData are string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order).\n\n\ninteger\nData are whole numbers (those numbers without a decimal point). To explicitly create an integer data type, use the suffix L (e.g. 2L).\n\n\nnumeric (also called double)\nData are numbers that contain a decimal.\n\n\n\n\nLess common data types (we won’t be going into these data types this course)\n\n\n\n\n\n\nData Type\nDefinition\n\n\n\n\ncomplex\nData are complex numbers with real and imaginary parts.\n\n\nraw\nData are raw bytes.\n\n\n\nWe’ve been using primarily integer or numeric data types so far. Let’s create an object that has a string value or a character data type.\n\nscience_rocks &lt;- \"yes it does!\"\n\n“yes it does!” is a string, and R knows it’s a word and not a number because it has quotes \" \". You can work with strings in your data in R easily thanks to the stringr and tidytext packages.\nThis lead us to an important concept in programming: As we now know, there are different “classes” or types of objects in R. The operations you can do with an object will depend on what type of object it is because each object has their own specialized format, designed for a specific purpose. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings).\nAlso, everything in R is an object. An object is a variable, function, data structure, or method that you have written to your environment.\nTry running the following line in your script:\n\n\"Hello world!\" * 3\n\nWhat happened? What do you see in the Console? Why?\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can see what data type or class an object is using the class() function, or you can use a logical test such as: is.numeric(), is.character(), is.logical(), and so on.\n\nclass(science_rocks) # returns character\nis.numeric(science_rocks) # returns FALSE\nis.character(science_rocks) # returns TRUE",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#data_structures",
    "href": "session_08.html#data_structures",
    "title": "Working in R & RStudio",
    "section": "8.6 Data structures in R",
    "text": "8.6 Data structures in R\nOkay, now let’s talk about vectors.\nA vector is the most common and most basic data structure in R. Vectors can be thought of as a way R stores a collection of values or elements. Think back to our weight_lb vector. That was a vector of three elements each with a data type or class of numeric.\nWhat we’re describing is a specific type of vector called atomic vectors. To put it simply, atomic vectors only contain elements of the same data type. Atomic vectors are very common.\nVectors are foundational for other data structures in R, including data frames, and while we won’t go into detail about other data structures there are great resources online that do. We recommend the chapter Vectors from the online book Advanced R by Hadley Wickham.\n\n# atomic vector examples #\n# character vector\nchr_vector &lt;- c(\"hello\", \"good bye\", \"see you later\")\n# numeric vector\nnumeric_vector &lt;- c(5, 1.3, 10)\n# logical vector\nboolean_vector &lt;- c(TRUE, FALSE, TRUE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#r-functions",
    "href": "session_08.html#r-functions",
    "title": "Working in R & RStudio",
    "section": "8.7 R Functions",
    "text": "8.7 R Functions\nSo far we’ve learned some of the basic syntax and concepts of R programming, and how to navigate RStudio, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in!\nA function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions.\nAll functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. These “needs” are pieces of information called arguments, and are required to return an expected value.\n\n\n\n\n\n\nSyntax of a function will look something like:\n\n\n\nresult_value &lt;- function_name(argument1 = value1, argument2 = value2, ...)\n\n\nBefore we use a function, let’s talk about Help pages.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#getting-help-using-help-pages",
    "href": "session_08.html#getting-help-using-help-pages",
    "title": "Working in R & RStudio",
    "section": "8.8 Getting help using help pages",
    "text": "8.8 Getting help using help pages\nWhat if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions.\nThe next function we’re about to use is the mean() function.\nTo access the help page for mean(), enter the following into your console:\n\n?mean\n\nThe Help pane will show up in the lower right hand corner of your RStudio.\nThe Help page is broken down into sections:\n\nDescription: An extended description of what the function does.\nUsage: The arguments of the function(s) and their default values.\nArguments: An explanation of the data each argument is expecting.\nDetails: Any important details to be aware of.\nValue: The data the function returns.\nSee Also: Any related functions you might find useful.\nExamples: Some examples for how to use the function.\n\nAnd there’s also help for when you only sort of remember the function name: double-question mark:\n\n??install \n\n\n\n\n\n\n\nNot all functions have (or require) arguments\n\n\n\nCheck out the documentation or Help page for date().\n\n?date()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#examples-using-built-in-r-functions-mean-and-read.csv",
    "href": "session_08.html#examples-using-built-in-r-functions-mean-and-read.csv",
    "title": "Working in R & RStudio",
    "section": "8.9 Examples using built-in R functions mean() and read.csv()",
    "text": "8.9 Examples using built-in R functions mean() and read.csv()\n\n8.9.1 Use the mean() function to run a more complex calculation\nLet’s override our weight object with some new values, and this time we’ll assign it three dog weights in pounds:\n\nweight_lb &lt;- c(55, 25, 12)\n\n\n\n\n\n\n\nExercise: Use the mean() function to calculate the mean weight.\n\n\n\nFrom the its Help page, we learned this function will take the mean of a set of numbers. Very convenient!\nWe also learned that mean() only has one argument we need to supply a value to (x). The rest of the arguments have default values.\n\n\n\n\nCode\nmean(x = weight_lb)\n\n\n[1] 30.66667\n\n\n\n\n\n\n\n\nExercise: Save the mean to an object called mean_weight_lb\n\n\n\nHint: What operator do we use to save values to an object?\n\n\n\n\nCode\n# saving the mean using the assignment operator `&lt;-`\nmean_weight_lb &lt;- mean(x = weight_lb)\n\n\n\n\n\n\n\n\nExercise: Update weight_lb\n\n\n\nLet’s say each of the dogs gained 5 pounds and we need to update our vector, so let’s change our object’s value by assigning it new values.\n\n\n\n\nCode\nweight_lb &lt;- c(60, 30, 17)\n\n\nCall mean_weight_lb in the console or take a look at your Global Environment. Is that the value you expected? Why or why not?\nIt wasn’t the value we expected because mean_weight_lb did not change. This demonstrates an important R programming concept: Assigning a value to one object does not change the values of other objects in R.\nNow that we understand why the object’s value hasn’t changed - how do we update the value of mean_weight_lb? How is an R Script useful for this?\nThis lead us to another important programming concept, specifically for R Scripts: An R Script runs top to bottom.\nThis order of operations is important because if you are running code line by line, the values in object may be unexpected. When you are done writing your code in an R Script, it’s good practice to clear your Global Environment and use the Run button and select “Run all” to test that your R Script successfully runs top to bottom.\n\n\n8.9.2 Use the read.csv() function to read a file into R\nSo far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv(), which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R.\nSince this is our first time using this function, first access the help page for read.csv(). This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file!\n\n\n\n\n\n\nDownload a file from the Arctic Data Center\n\n\n\n\nNavigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.\nDownload the first csv file called BGchem2008data.csv by clicking the “download” button next to the file.\nClick the “Upload” button in your RStudio server file browser.\nIn the dialog box, make sure the destination directory is the data directory in your R project, click “Choose File,” and locate the BGchem2008data.csv file. Press “OK” to upload the file.\nCheck your file was successfully uploaded by navigating into your data folder in the Files pane.\n\n\n\nNow we have to tell read.csv() how to find the file. We do this using the file argument which you can see in the usage section in the help page. In R, you can either use absolute paths (which will start with your home directory ~/) or paths relative to your current working directory. RStudio has some great auto-complete capabilities when using relative paths, so we will go that route.\nAssuming you have moved your file to a folder within training_{USERNAME} called data, and your working directory is your project directory (training_{USERNAME}) your read.csv() call will look like this:\n\n# reading in data using relative paths\nbg_chem_dat &lt;- read.csv(file = \"data/BGchem2008data.csv\")\n\nYou should now have an object of the class data.frame in your environment called bg_chem_dat. Check your environment pane to ensure this is true. Or you can check the class using the function class() in the console.\n\n\n\n\n\n\nOptional Arguments\n\n\n\nNotice that in the Help page there are many arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required.\nOptional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv()).\nRequired arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv() is file.\n\n\nYou can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = \"data/BGchem2008data.csv\", since file is the first argument.\nIf we explicitly called the file argument our code would like this:\n\nbg_chem_dat &lt;- read.csv(file = \"data/BGchem2008data.csv\")\n\nIf we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header.\nMany R users (including myself) will set the stringsAsFactors argument using the following call:\n\n# relative file path\nbg_chem_dat &lt;- read.csv(\"data/BGchem2008data.csv\", stringsAsFactors = FALSE)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nFor functions that are used often, you’ll see many programmers will write code that does not explicitly call the first or second argument of a function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#working-with-data-frames-in-r-using-the-subset-operator",
    "href": "session_08.html#working-with-data-frames-in-r-using-the-subset-operator",
    "title": "Working in R & RStudio",
    "section": "8.10 Working with data frames in R using the Subset Operator $",
    "text": "8.10 Working with data frames in R using the Subset Operator $\nA data.frame is a list data structure in R that can represent tables and spreadsheets – we can think of it as a table. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents an observation containing a measurement of that variable. When we ran read.csv(), the object bg_chem_dat that we created was a data.frame. The columns in a data.frame might represent measured numeric response values (e.g., weight_kg), classifier variables (e.g., site_name), or categorical response variables (e.g., course_satisfaction). There are many ways R and RStudio help you explore data frames. Here are a few, give them each a try:\n\nClick on the word bg_chem_dat in the environment pane\nClick on the arrow next to bg_chem_dat in the environment pane\nExecute head(bg_chem_dat) in the Console\nExecute View(bg_chem_dat) in the Console\n\nUsually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $.\nSay you want to look at the first few rows of the Date column only:\n\nhead(bg_chem_dat$Date)\n\nYou can also use the subset operator $ calculations. For example, let’s calculated the mean temperature of all the CTD samples.\n\nmean(bg_chem_dat$CTD_Temperature)\n\nYou can also save this calculation to an object that was created using the subset operator $.\n\nmean_temp &lt;- mean(bg_chem_dat$CTD_Temperature)\n\n\n\n\n\n\n\nOther ways to load tablular data\n\n\n\nWhile the base R package provides read.csv as a common way to load tabular data from text files, there are many other ways that can be convenient and will also produce a data.frame as output. Here are a few:\n\nUse the readr::read_csv() function from the Tidyverse to load the data file. The readr package has a bunch of convenient helpers and handles CSV files in typically expected ways, like properly typing dates and time columns. bg_chem_dat &lt;- readr::read_csv(\"data/BGchem2008data.csv\")\nLoad tabular data from Excel spreadsheets using the readxl::read_excel() function.\nLoad tabular data from Google Sheets using the googlesheets4::read_sheet() function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#error-messages-are-your-friends",
    "href": "session_08.html#error-messages-are-your-friends",
    "title": "Working in R & RStudio",
    "section": "8.11 Error messages are your friends",
    "text": "8.11 Error messages are your friends\nThere is an implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type.\nRemember that this is a language, not dissimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. Note that knowing how to Google is a skill and takes practice - use our Masters of Environmental Data Science (MEDS) program workshop Teach Me How to Google as a guide.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#r-packages",
    "href": "session_08.html#r-packages",
    "title": "Working in R & RStudio",
    "section": "8.12 R Packages",
    "text": "8.12 R Packages\n\n\n\nArtwork by Allison Horst\n\n\nR packages are the building blocks of computational reproducibility in R. Each package contains a set of related functions that enable you to more easily do a task or set of tasks in R. There are thousands of community-maintained packages out there for just about every imaginable use of R - including many that you have probably never thought of!\nTo install a package, we use the syntax install.packages(\"packge_name\"). A package only needs to be installed once, so this code can be run directly in the console if needed. Generally, you don’t want to save your install package calls in a script, because when you run the script it will re-install the package, which you only need to do once, or if you need to update the package.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#r-resources",
    "href": "session_08.html#r-resources",
    "title": "Working in R & RStudio",
    "section": "8.13 R Resources",
    "text": "8.13 R Resources\n\nAwesome R Resources to Check out\n\n\n\n\n\n\nLearning R Resources\n\nIntroduction to R lesson in Data Carpentry’s R for data analysis course\nJenny Bryan’s Stat 545 course materials\nJulie Lowndes’ Data Science Training for the Ocean Health Index\nLearn R in the console with swirl\nProgramming in R\nR, RStudio, RMarkdown\n\n\n\nCommunity Resources\n\nNCEAS’ EcoDataScience\nR-Ladies\nrOpenSci\nMinorities in R (MiR)\nTwitter - there is a lot here but some hashtags to start with are:\n\n#rstats\n#TidyTuesday\n#dataviz\n\n\n\n\nCheatsheets\n\nBase R Cheatsheet\nLaTeX Equation Formatting\nMATLAB/R Translation Cheatsheet",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#clearing-the-environment",
    "href": "session_08.html#clearing-the-environment",
    "title": "Working in R & RStudio",
    "section": "8.14 Clearing the environment",
    "text": "8.14 Clearing the environment\nTake a look at the objects in your Environment (Workspace) in the upper right pane. The Workspace is where user-defined objects accumulate. There are a few useful commands for getting information about your Environment, which make it easier for you to reference your objects when your Environment gets filled with many, many objects.\n\nYou can get a listing of these objects with a couple of different R functions:\n\nobjects()\n\n[1] \"boolean_vector\"  \"chr_vector\"      \"important_value\" \"mean_weight_lb\" \n[5] \"numeric_vector\"  \"science_rocks\"   \"weight_kg\"       \"weight_lb\"      \n\nls()\n\n[1] \"boolean_vector\"  \"chr_vector\"      \"important_value\" \"mean_weight_lb\" \n[5] \"numeric_vector\"  \"science_rocks\"   \"weight_kg\"       \"weight_lb\"      \n\n\nIf you want to remove the object named weight_kg, you can do this:\n\nrm(weight_kg)\n\nTo remove everything (or click the Broom icon in the Environment pane):\n\nrm(list = ls())\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nIt’s good practice to clear your environment. Over time your Global Environmental will fill up with many objects, and this can result in unexpected errors or objects being overridden with unexpected values. Also it’s difficult to read / reference your environment when it’s cluttered!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#save-workspace-image-to-.rdata",
    "href": "session_08.html#save-workspace-image-to-.rdata",
    "title": "Working in R & RStudio",
    "section": "8.15 Save Workspace Image to .RData?",
    "text": "8.15 Save Workspace Image to .RData?\n\nDON’T SAVE\nWhen ever you close or switch projects you will be promped with the question: Do you want to save your workspace image to /“currente-project”/ .RData?\nRStudio by default wants to save the state of your environment (the objects you have in your environment pane) into the RData file so that when you open the project again you have the same environment. However, as we discussed above, it is good practice to constantly clear and clean your environment. It is generally NOT a good practice to rely on the state of your environment for your script to run and work. If you are coding reproducibly, your code should be able to reproduce the state of your environment (all the necessary objects) every time you run it. It is much better to rely on your code recreating the environment than the saving the workspace status.\nYou can change the Global Options configuration for the default to be NEVER SAVE MY WORKSPACE. Go to Tools &gt; Global Options. Under the General menu, select Never next to “Save workspace to .RData on exit”. This way you won’t get asked every time you close a project, instead RStudio knows not to save.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_08.html#logical-operators-and-expressions",
    "href": "session_08.html#logical-operators-and-expressions",
    "title": "Working in R & RStudio",
    "section": "8.16 Logical operators and expressions",
    "text": "8.16 Logical operators and expressions\nWe can ask questions about an object using logical operators and expressions. Let’s ask some “questions” about the weight_lb object we made.\n\n== means ‘is equal to’\n!= means ‘is not equal to’\n&lt; means ‘is less than’\n&gt; means ‘is greater than’\n&lt;= means ‘is less than or equal to’\n&gt;= means ‘is greater than or equal to’\n\n\n# examples using logical operators and expressions\nweight_lb == 2\nweight_lb &gt;= 30\nweight_lb != 5",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working in R & RStudio</span>"
    ]
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "Literate Analysis with Quarto",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "Literate Analysis with Quarto",
    "section": "",
    "text": "Introduce literate analysis using Quarto (an extension of RMarkdown’s features)\nLearn markdown syntax and run R code using Quarto\nBuild and render an example analysis",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#introduction-to-literate-programming",
    "href": "session_09.html#introduction-to-literate-programming",
    "title": "Literate Analysis with Quarto",
    "section": "9.1 Introduction to Literate Programming",
    "text": "9.1 Introduction to Literate Programming\nAll too often, computational methods are written in such a way as to be borderline incomprehensible even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. In 1984, Donald Knuth proposed a reversal of the programming paradigm by introducing the concept of Literate Programming (Knuth 1984).\n\n“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”\n\nIf our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding.\nQuarto and RMarkdown are an excellent way to generate literate analysis, and a reproducible workflow. These types of files, combine R the programming language, and markdown, a set of text formatting directives.\nIn an R script, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown or Quarto, where instead of assuming you are writing code, they assume that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader.\nBefore we dive in deeper, let’s look at an example of what a rendered literate analysis can look like using a real example. Here is an example of an analysis workflow written using RMarkdown. Note that if this analysis would be in Quarto, the render version it would be similar, except for formatting and layout (eg: the default font in Quarto is different).\nThere are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source.\n\nIt introduces the data sources using in-line images, links, interactive tables, and interactive maps.\nAn example of data formatting from one source using R is shown.\nThe document executes a set of formatting scripts in a directory to generate a single merged file.\nSome simple quality checks are performed (and their output shown) on the merged data.\nSimple analysis and plots are shown.\n\nIn addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the Quarto file, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data.\n\n\n\n\n\n\nA note on reproducibility\n\n\n\nReproducible analysis allow you to automatize how the figures and the statistics in your analysis are generated. This process also helps your collaborators, your readers and your future self to follow your code trail the leads to the original data, increasing the transparency of your science.\nLiterate analysis help reduce the mistakes from copying and pasting across software, keeps results and models in sync, and allows you to provide interested readers with more information about the different approaches and analyses you tried before coming up with the final results (British Ecological Society (2017)).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#rmarkdown-and-quarto",
    "href": "session_09.html#rmarkdown-and-quarto",
    "title": "Literate Analysis with Quarto",
    "section": "9.2 RMarkdown and Quarto",
    "text": "9.2 RMarkdown and Quarto\nYou can identify a Quarto file with the .qmd extension. On the other hand, an RMarkdown file has a .Rmd extension. Both have similar structures and both combine prose with code.Quarto provides a rich support to languages other than R such as Python, Observable, and Julia. It also excels in formatting and layout. Allowing users to customize in details the looks of the rendered documents. On the other hand, RMarkdown is compatible with some languages that Quarto is not, for example bash. Quarto and Rmarkdown are amazing tools to use for collaborative research. During this course e will spend some time learning and using the basics of Quarto and provide some comparisons to RMarkdown.\n\nNow, let’s take a look at the structure of each of these files. The both look for the most part the same with minor differences.\n\n\nFinally, lets compare each of these files when knitted/rendered.\n\n\nAgain, we see similar outcomes, with minor differences mainly in formatting (font, style of showing code chunks, etc.)\nBoth type of documents have three main components:\n\nYAML metadata to guide the document’s build process\nCode chunks to run\nProse (Text to display)\n\nToday we are going to use Quarto to run some analysis on data. We are specifically going to focus on the code chunk and text components. We will discuss more about the how the YAML works in an Quarto later in the course.\n\n\n\n\n\n\nThe YAML\n\n\n\nIs the document’s metadata which sets guidelines on how your want the output of your document to look like. It is located at the top of your file, delineated by three dashes (---) at the top and at the bottom of it. It can be used to specify:\n\nCharacteristics of your documents such at title, author, date of creation.\nArgument to pass on the building process to control the format of the output.\nAdd additional information such as the bibliography file (and formatting of the references)\nSpecific parameters for your report (eg: just used a subset of the data).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#a-quarto-document",
    "href": "session_09.html#a-quarto-document",
    "title": "Literate Analysis with Quarto",
    "section": "9.3 A Quarto Document",
    "text": "9.3 A Quarto Document\nLet’s open an Quarto file following the instructions below.\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen a new Quarto file using the following prompts: File &gt; New File &gt; Quarto Document\nA popup window will appear.\nGive your file a new title, e.g “Introduction to Quarto”.\nLeave the output format as HTML and Engine set to Knitr.\nThen click the “Create” button.\n\n\n\nThe first thing to notice is that by opening a file, we see the fourth pane of the RStudio pops up. This is our Quarto document which is essentially a text editor. We also see in the upper left side that we are looking at the document under the “Visual editor”. This is probably a familiar way of looking at a text document. To introduce the markdown syntax, we re going to move to the source editor and then come back to the visual editor. In the upper left corner, click on Source. See how the formatting changed? In the Source editor we are looking at the same text, but in markdown syntax. The visual editor on the other hand, allows us to see how markdown is rendered, therefore how is it going to look in our output document.\nLet’s have a look at this file — As we saw in the examples above, it looks a little different than a R script. It’s not blank; there is some initial text already provided for you. Lets identify the three main components we introduces before. We have the YAML a the top, in between the two sets of dashed lines. Then we also see white and grey sections. The gray sections are R code chunks and the white sections are plain text.\nLet’s go ahead and render this file by clicking the “Render” button, next to the blue arrow at the top of the Quarto file. When you first click this button, RStudio will prompt you to save this file. Save it in into your scripts folder, and name it something that you will remember (like quarto-intro.Rmd).\n\n\n\nWhat do you notice between the two?\nFirst, the render process produced a second file (an HTML file) that popped up in a second window in the browser. You’ll also see this file in your directory with the same name as your qmd, but with the .html extension. In it’s simplest format, Quarto files come in pairs (same than RMarkdown files) the Quarto document, and its rendered version. In this case, we are rendering, the file into HTML. You can also knit to PDF or Word files and others.\nNotice how the grey R code chunks are surrounded by 3 back-ticks and {r LABEL}. The first chunk, in this case 1+1, is evaluated and return the output number (2). Notice the line in the second chunk that says #| echo: false? This is a code chunk option that indicates not to print the code. In the rendered version, we can see the outcome of 2*2 but not the executed code that created the outcome.\nThe table below show some of the options available to customizing outputs (Quarto.org).\n\nCode chunk options\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n#| eval:\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\n#| echo:\nInclude the source code in output\n\n\n#| warning:\nInclude warnings in the output.\n\n\n#| error:\nInclude warnings in the output.\n\n\n#| include:\nCatch all for preventing any output (code or results) from being included (e.g.include: false suppresses all output from the code block).\n\n\n\nNote that you can also combine these options by adding more than one to a code chunk.\n\n\n\n\n\n\nImportant\n\n\n\nOne important difference between Quarto documents and RMarkdown documents is that in Quarto, chunk options are written in special comment format (#|) at the top of code chunks rather than within the wiggly brackets next to ```{r} at the begging of the chunk. For example:\n\nQuarto code options syntax\n\n\n\nRMarkdown code options syntax\n\n\n\n\nIt is important to emphasize one more time that in an Quarto (and RMarkdown) document, the gray areas of the document are code, in this case R code because that is what it is indicated in the ```{r} syntax at the start of this gray area. And the white areas of a qmd are in markdown language.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#markdown-syntax",
    "href": "session_09.html#markdown-syntax",
    "title": "Literate Analysis with Quarto",
    "section": "9.4 Markdown Syntax",
    "text": "9.4 Markdown Syntax\nLet’s start by talking about markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know.\nNotice the syntax in the document we just knitted:\n\nHeaders get rendered at multiple levels: #, ##\nBold: **word**\n\nThere are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference.\n\n\n\n\n\n\nImportant\n\n\n\nThe hash symbol # is used differently in markdown and in R\n\nIn an R script or inside an R code chunk, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style.\nIn markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nIn markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference).\nRe-knit your html file and observe your edits.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#the-visual-editor",
    "href": "session_09.html#the-visual-editor",
    "title": "Literate Analysis with Quarto",
    "section": "9.5 The Visual Editor",
    "text": "9.5 The Visual Editor\nQuarto has a “what you see is what you mean” (WYSIWYM) editor or Visual editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, we recommend just learning them especially since markdown is used in many, many other contexts besides Quarto and RMarkdown. For example, formatting GitHub comments and README files.\nTo access the editor, click the Visual button in the upper left hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although we don’t really recommend doing all of your markdown composition in the Visual editor, there are two features to this editor that we believe are immensely helpful, adding citations, and adding tables.\n\n9.5.1 Adding citations\nTo add a citation, go to the visual editor and in the insert drop down, select “Citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you have a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (e.g.: 10.1038/s41467-020-17726-z), then select “Insert.”\n\nAfter selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your Quarto document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy!\n\n\n9.5.2 Adding table in markdown\nThe second task that the visual editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “Table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#code-chunks-in-quarto",
    "href": "session_09.html#code-chunks-in-quarto",
    "title": "Literate Analysis with Quarto",
    "section": "9.6 Code Chunks in Quarto",
    "text": "9.6 Code Chunks in Quarto\nEvery time when opening a new Quarto document we should start by deleting all template text (everything except for the YAML). Then we save the document into the most convenient folder of our project. Now we are ready to start our work.\nYou can create a new chunk in your Quarto in one of these ways:\n\nGo to Code in the top menu bar, click “Insert Chunk”\nType by hand {r}\nUse the keyboard shortcut\n\nMac:command + option + i\nWindows: Ctrl + Alt + i\n\n\n\n\n\n\n\n\nAbout code chunks\n\n\n\nEach code chunk needs to have an opening syntax ```{r} and a closing syntax ```. Everything in between these lines will be identified as R code.\n\n\nIf I want to write some R code, this is how it would look like.\n\nx &lt;- 4 * 8\n\nhights_ft &lt;- c(5.2, 6.0, 5.7)\n\ncoef &lt;- 3.14\n\nHitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them):\n\nCopy-paste this line into the console (generally not recommended as a primary method)\nSelect the line (or simply put the cursor there), and click “Run”. This is available from:\n\nthe bar above the file (green arrow)\nthe menu bar: Code &gt; Run Selected Line(s)\nkeyboard shortcut: command-return\n\nClick the green arrow at the right of the code chunk",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#practice-literate-analysis-with-ocean-water-samples",
    "href": "session_09.html#practice-literate-analysis-with-ocean-water-samples",
    "title": "Literate Analysis with Quarto",
    "section": "9.7 Practice: Literate Analysis with ocean water samples",
    "text": "9.7 Practice: Literate Analysis with ocean water samples\nNow that we have gone over the basics, let’s go a little deeper by building a simple, Quarto document that represents a literate analysis using real data. We are going to work with the seawater chemistry data. We are going to use the BGchem2008data.csv data we downloaded in our previous session.\n\n\n9.7.1 Getting Started\n\n\n\n\n\n\n\n\nExperienced R users who have never used Quarto (or RMarkdown) often struggle a bit in the transition to developing analysis in Prose+Code format — which makes sense! It is switching the code paradigm to a new way of thinking.\nRather than starting an R chunk and putting all of your code in that single chunk, below we describe what we think is a better way.\n\nOpen a document and block out the high-level sections you know you’ll need to include using top level headers.\nAdd bullet points for some high level pseudo-code steps you know you’ll need to take.\nStart filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed.\n\nFor this mini-analysis, we will have the following sections and code steps:\n\nIntroduction\n\nAbout the data\nSetup\nRead in data\n\nAnalysis\n\nCalculate summary statistics\nCalculate mean Redfield ratio\nPlot Redfield ratio\n\nConclusion\n\n\n\n\n\n\n\nExercise\n\n\n\nUnder “About the data”, write a sentence saying where the data set came from, including a hyperlink ti the data. Also mention when was the data downloaded.\nHint: Navigate to Help &gt; Markdown Quick Reference to look-up the hyperlink syntax.\n\n\n\n\n9.7.2 Read in the data\nNow that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package.\nTo use a package in our analysis, we need to first make sure it is installed (you can install a package by running install.package(\"name-of-package\")). Once installed you need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our literate analysis, so we will include it as an R chunk as part of our Setup.\n\n\n\n\n\n\nBest Practice\n\n\n\nIt is generally good practice to include all of your library() calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code.\n\n\nThe server should have already installed the two packages we need for now: readr and here. Let’s add a new R chunk below your Setup header that calls these libraries, and run it.\nIt should look like this:\n\nlibrary(readr)\nlibrary(here)\n\n\n\n\n\n\n\nQuarto file path and the here() function\n\n\n\nQuarto has a special way of handling relative paths that can be very handy. When working in an Quarto document, R will set all paths relative to the location of the Quarto file. This can make things easier to read in data if your Quarto document is stored in the same directory or “near” by. However, more often that not, your .qmd file will be stored in a a folder (e.g scripts) and your data in a data folder, (both folder in the main project directory).\nThe here() function helps navigate this file path mix up in a straight forward and reproducible way. This function sets the file path to the project’s directory and builds the rest of the file path from there. Making it easier to find files inside different folders in a project. In this case, because the .qmd file lives in the script folder, here() makes is easy to navigate back into the project’s directory and then into the data folder to read in our file.\n\n\nNow, under “Read data”, add a code chunk that uses the read_csv() with the here() function to read in your data file.\n\n\nRows: 70 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Station\ndbl  (16): Latitude, Longitude, Target_Depth, CTD_Depth, CTD_Salinity, CTD_T...\ndttm  (1): Time\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbg_chem &lt;- read_csv(here::here(\"data/BGchem2008data.csv\"))\n\n\n\nWhy read_csv() over read.csv()?\nWe chose to show read_csv() from the readr package to introduce the concept of packages, to show you how to load packages, and read_csv() has several advantages over read.csv() from base R, including:\n\nMore reasonable function defaults (no stringsAsFactors!)\nSmarter column type parsing, especially for dates\nread_csv() is much faster than read.csv(), which is helpful for large files\n\nOnce you run this line in your document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document.\n\n\n\n\n\n\nExercise\n\n\n\nHow would you suppress the warnings (so they don’t show in our output file) form a specific code chunk?\nHint: Code chunk options\n\n\n\n\n9.7.3 Calculate Summary Statistics\nAs our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. Using water samples from the Arctic Ocean, we will examine the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton.\nLet’s start by exploring the data we just read. Every time we read a new data set, it is important to familiarize yourself with it and make sure that the data looks as expected. Below some useful functions for exploring your data.\nLet’s start by creating a new R chunk and run the following functions. Because this just an exploration and we do not want this chunk to be part of our report, we will indicate that by adding #|eval: false and #| echo: false in the setup of the chunk, that way, the code in this chunk will not run and not be displayed when I knit the final document.\n\n## Prints the column names of my data frame\ncolnames(bg_chem)\n\n## General structure of the data frame - shows class of each column\nstr(bg_chem)\n\n## First 6 lines of the data frame\nhead(bg_chem)\n\n## Summary of each column of data\nsummary(bg_chem)\n\n## Prints unique values in a column (in this case Date)\nunique(bg_chem$Date)\n\nTo peek out data frame, we can type View(bg_chem) in the console. This will open a tab with our data frame in a tabular format.\nNow that we know a more about the data set we are working with lets do some analyses. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured.\nSave these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. Remember that the $ (aka the subset operator) indicates which column of your data to look into.\n\nnitrate &lt;- mean(bg_chem$NO3)\nnitrite &lt;- mean(bg_chem$NO2)\namm &lt;- mean(bg_chem$NH4)\nphos &lt;- mean(bg_chem$P)\n\nIn another chunk, use those variables to calculate the nitrogen: phosphate ratio (Redfield ratio).\n\nratio &lt;- (nitrate + nitrite + amm)/phos\n\nYou can access this variable in your markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single back tick `, followed by the letter “r”, then whatever your simple R command is — here we will use round(ratio) to print the calculated ratio, and finally a closing back tick `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task.\nSo, the text in you Quarto document should look like this:\nThe Redfield ratio for this dataset is approximately: `r round(ratio)`\nAnd the rendered text like this:\nThe Redfield ratio for this dataset is approximately 6.\nFinally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio.\n\nplot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDecide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option.\nRender your Quarto document (by pressing the Render button) and observe the results.\n\n\n\n\n\n\n\n\nHow do I decide when to make a new code chunk?\n\n\n\nLike many of life’s great questions, there is no clear cut answer. A rule of thumb is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as we’ll see below). Ultimately, the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#quarto-and-environments",
    "href": "session_09.html#quarto-and-environments",
    "title": "Literate Analysis with Quarto",
    "section": "9.8 Quarto and Environments",
    "text": "9.8 Quarto and Environments\nLet’s walk through an exercise with the document we just created to demonstrate how Quarto handles environments. We will be deliberately inducing some errors here for demonstration purposes.\nFirst, follow these steps:\n\n\n\n\n\n\nSetup\n\n\n\n\nRestart your R session (Session &gt; Restart R)\nRun the last chunk in your Quarto document by pressing the play button on the chunk\n\n\n\nPerhaps not surprisingly, we get an error:\nError in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : \n  object 'bg_chem' not found\nThis is because we have not run the chunk of code that reads in the bg_chem data. The R part of Quarto works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large Quarto document — running chunks out of order, or forgetting to run chunks.\nTo resolve this, follow the next step:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of the editor pane) “Run All.”\nObserve the bg_chem variable in your environment\n\n\n\nThis is a great way to reset and re-run code when things seem to have gone sideways. It is great practice to do periodically since it helps ensure you are writing code that actually runs and it’s reproducible.\n\n\n\n\n\n\nFor the next exercise:\n\n\n\n\nClean your environment by clicking the broom in the environment pane\nRestart your R session (Session &gt; Restart R)\nPress “Render” to run all of the code in your document\nObserve the state of your environment pane\n\nAssuming your document rendered and produced an html page, your code ran. Yet, the environment pane is empty. What happened?\n\n\nThe Render button is rather special — it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways Quarto (or RMarkdown) helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your Quarto document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library() calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script.\nTo further clarify the point on environments, perform the following steps:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of editor pane) “Run All”\nObserve all of the variables in your environment\n\n\n\n\n\n\n\n\n\nWhat about all my R Scripts?\n\n\n\nSome pieces of R code are better suited for R scripts than Quarto or RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many Quarto or RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary Quarto document, the authors chose to write one R script per file and stored them all in a directory. Then, took the contents of one script and included it in the literate analysis, using it as an example to explain what the scripts did, and then used the source() function to run them all from within the Quarto document.\nSo, just because you know Quarto now, doesn’t mean you won’t be using R scripts anymore. Both .R and .qmd have their roles to play in analysis. With practice, it will become more clear what works well in Quarto or RMarkdown, and what belongs in a regular R script.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#additional-quarto-resources",
    "href": "session_09.html#additional-quarto-resources",
    "title": "Literate Analysis with Quarto",
    "section": "9.9 Additional Quarto Resources",
    "text": "9.9 Additional Quarto Resources\n\nPosit (the organization that developed Quarto) has great documentation, check out Quarto.org\nR for Data Science (2e) (Wickham et al, 2023), this is an awesome book for all R related things. Chapter 29 and 30 are specific to Quarto.\nQuarto Gallery: Example of different outputs created using Quarto\nHello Quarto: share, collaborate, teach, reimagine. A talk by Julia Stewart Lowndes and Mine Cetinkaya-Runde.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_09.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "href": "session_09.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "title": "Literate Analysis with Quarto",
    "section": "9.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF",
    "text": "9.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF\nIf you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening:\n\nYour computer doesn’t have LaTeX installed\nYou have an installation of LaTeX but RStudio cannot find it (it is not on the path)\n\nIf you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here.\nIf you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer.\nTo install tinytex run:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal:\nsudo chown -R `whoami`:admin /usr/local/bin\nand then try the above install instructions again. Learn more about tinytex from Yihui Xie’s online book TinyTeX. ````\n\n\n\n\nBritish Ecological Society, Mike, Croucher. 2017. “A Guide to Reproducible Code in Ecology and Evolution.” British Ecological Society. https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "Cleaning and Wrangling Data",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#learning-objectives",
    "href": "session_10.html#learning-objectives",
    "title": "Cleaning and Wrangling Data",
    "section": "",
    "text": "Introduce dplyr and tidyr functions to clean and wrangle data for analysis\nLearn about the Split-Apply-Combine strategy and how it applies to data wrangling\nDescribe the difference between wide vs. long table formats and how to convert between them",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#introduction",
    "href": "session_10.html#introduction",
    "title": "Cleaning and Wrangling Data",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nThe data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do further work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly. Learning these tools well will greatly increase your efficiency as an analyst.\nLet’s look at two motivating examples.\n\n\n\n\n\n\nExample 1\n\n\n\nSuppose you have the following data.frame called length_data with data about salmon length and want to calculate the average length per year.\n\n\n\nyear\nlength_cm\n\n\n\n\n1990\n5.673318\n\n\n1991\n3.081224\n\n\n1991\n4.592696\n\n\n1992\n4.381523\n\n\n1992\n5.597777\n\n\n1992\n4.900052\n\n\n\nBefore thinking about the code, let’s think about the steps we need to take to get to the answer (aka pseudocode).\nNow, how would we code this? The dplyr R library provides a fast and powerful way to do this calculation in a few lines of code:\n\n\nAnswer\nlength_data %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_length_cm = mean(length_cm))\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nAnother process we often need to do is to “reshape” our data. Consider the following table that is in what we call “wide” format:\n\n\n\nsite\n1990\n1991\n…\n1993\n\n\n\n\ngold\n100\n118\n…\n112\n\n\nlake\n100\n118\n…\n112\n\n\n…\n…\n…\n…\n…\n\n\ndredge\n100\n118\n…\n112\n\n\n\nYou are probably familiar with data in the above format, where values of the variable being observed are spread out across columns. In this example we have a different column per year. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R (and know it is not tidy data!). For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run lm(length ~ year). But this won’t work on our wide data because lm() needs length and year to be columns in our table.\nWhat steps would you take to get this data frame in a long format?\nThe tidyr package allows us to quickly switch between wide format and long format using the pivot_longer() function:\n\n\nAnswer\nsite_data %&gt;% \n  pivot_longer(-site, \n               names_to = \"year\", \n               values_to = \"length\")\n\n\n\n\n\nsite\nyear\nlength\n\n\n\n\ngold\n1990\n101\n\n\nlake\n1990\n104\n\n\ndredge\n1990\n144\n\n\n…\n…\n…\n\n\ndredge\n1993\n145\n\n\n\n\n\nThis lesson will cover examples to learn about the functions you’ll most commonly use from the dplyr and tidyr packages:\n\nCommon dplyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\nmutate()\nCreates modify and deletes columns\n\n\ngroup_by()\nGroups data by one or more variables\n\n\nsummarise()\nSummaries each group down to one row\n\n\nselect()\nKeep or drop columns using their names\n\n\nfilter()\nKeeps rows that matches conditions\n\n\narrange()\norder rows using columns variable\n\n\nrename()\nRename a column\n\n\n\n\nCommon tidyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\npivot_longer()\ntransforms data from a wide to a long format\n\n\npivot_wider()\ntransforms data from a long to a wide format\n\n\nunite()\nUnite multiple columns into one by pasting strings together\n\n\nseparate()\nSeparate a character column into multiple columns with a regular expression or numeric locations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#data-cleaning-basics",
    "href": "session_10.html#data-cleaning-basics",
    "title": "Cleaning and Wrangling Data",
    "section": "10.2 Data cleaning basics",
    "text": "10.2 Data cleaning basics\nTo demonstrate, we’ll be working with a tidied up version of a data set from Alaska Department of Fish & Game containing commercial catch data from 1878-1997. The data set and reference to the original source can be found at its public archive.\n\n\n\n\n\n\nSetup\n\n\n\nFirst, open a new Quarto document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\n\n\n\n\n\n\n\n\nA note on loading packages\n\n\n\nYou may have noticed the following messages pop up when you ran your library chunk.\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nThese are important messages. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter(), you mean the dplyr version unless you specify otherwise.\nBeing specific about which version of filter(), for example, you call is easy. To explicitly call a function by its unambiguous name, we use the syntax package_name::function_name(...). So, if we wanted to call the stats version of filter() in this Rmarkdown document, I would use the syntax stats::filter(...).\n\n\n\n\n\n\n\n\nRemove messages and warnings\n\n\n\nMessages and warnings are important, but we might not want them in our final document. After you have read the packages in, adjust the chunk settings in your library chunk to suppress warnings and messages by adding #| message: false or #| warning: false. Both of these chunk options, when set to false, prevents messages or warnings from appearing in the rendered file.\n\n\nNow that we have introduced some data wrangling libraries, let’s get the data that we are going to use for this lesson.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to KNB Data Package Alaska commercial salmon catches by management region (1886- 1997)\nFind the data file df35b.302.1. Right click the “Download” button and select “Copy Link Address”\nPaste the copied URL into the read_csv() function\n\nThe code chunk you use to read in the data should look something like this:\n\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\nNote for Windows users: Keep in mind, if you want to replicate this workflow in your local computer you also need to use the url() function here with the argument method = \"libcurl\".\nIt would look like this:\n\ncatch_original &lt;- read.csv(url(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\", method = \"libcurl\"))\n\n\n\nThis data set is relatively clean and easy to interpret as-is. While it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first.\n\n\n\n\n\n\nExercise\n\n\n\nBefore we get too much further, spend a minute or two outlining your Quarto document so that it includes the following sections and steps:\n\nData Sources\n\nRead in the data\nExplore data\n\nClean and Reshape data\n\nUsing select() function\nCheck column types\nReplace values in a column with mutate()\nReshape data with pivot_longer() and pivot_wider()\nRename columns rename()\nAdd columns with mutate()\nSummary stats using group_by() and summarize()\nFiltering rows using filter()\nSort data using arrange()\nSplit and combine values in columns with separate() and unite()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#data-exploration",
    "href": "session_10.html#data-exploration",
    "title": "Cleaning and Wrangling Data",
    "section": "10.3 Data exploration",
    "text": "10.3 Data exploration\nSimilar to what we did in our Literate Analysis lesson, it is good practice to skim through the data you just read in.\nDoing so is important to make sure the data is read as you were expecting and to familiarize yourself with the data.\nSome of the basic ways to explore your data are:\n\n## Prints the column names of my data frame\ncolnames(catch_original)\n\n## First 6 lines of the data frame\nhead(catch_original)\n\n## Summary of each column of data\nsummary(catch_original)\n\n## Prints unique values in a column (in this case, the region)\nunique(catch_original$Region)\n\n## Opens data frame in its own tab to see each row and column of the data (do in console)\nView(catch_original)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#about-the-pipe-operator",
    "href": "session_10.html#about-the-pipe-operator",
    "title": "Cleaning and Wrangling Data",
    "section": "10.4 About the pipe (%>%) operator",
    "text": "10.4 About the pipe (%&gt;%) operator\nBefore we jump into learning tidyr and dplyr, we first need to explain the pipeline operator %&gt;%.\nBoth the tidyr and the dplyr packages use the pipe operator (%&gt;%), which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement.\nSay you want to both filter() out rows of a data set, and select() certain columns.\nInstead of writing:\n\ndf_filtered &lt;- filter(df, ...)\ndf_selected &lt;- select(df_filtered, ...)\n\nYou can write:\n\ndf_cleaned &lt;- df %&gt;% \n    filter(...) %&gt;%\n    select(...)\n\nIf you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then”.\nSo you might think of the above chunk being translated as:\n\nThe cleaned data frame gets the original data, and then a filter (of the original data), and then a select (of the filtered data).\n\nThe benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it.\n\n\n\n\n\n\nQuick Tip\n\n\n\nRStudio has a keyboard shortcut for %&gt;%\n\nWindows: Ctrl + Shift + M\nMac: cmd + shift + M",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#selecting-or-removing-columns-using-select",
    "href": "session_10.html#selecting-or-removing-columns-using-select",
    "title": "Cleaning and Wrangling Data",
    "section": "10.5 Selecting or removing columns using select()",
    "text": "10.5 Selecting or removing columns using select()\nWe’re ready to go back to our salmon dataset. The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data.\n\ncatch_data &lt;- catch_original %&gt;%\n    select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum)\n\nhead(catch_data)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886 0             5     0     0     0\n2 SSE     1887 0           155     0     0     0\n3 SSE     1888 0           224    16     0     0\n4 SSE     1889 0           182    11    92     0\n5 SSE     1890 0           251    42     0     0\n6 SSE     1891 0           274    24     0     0\n\n\nMuch better!\nThe select() function also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs:\n\ncatch_data &lt;- catch_original %&gt;%\n    select(-All,-notesRegCode)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#quality-check",
    "href": "session_10.html#quality-check",
    "title": "Cleaning and Wrangling Data",
    "section": "10.6 Quality check",
    "text": "10.6 Quality check\nNow that we have the data we are interested in using, we should do a little quality check to see that everything seems as expected. One nice way of doing this is the glimpse() function.\n\ndplyr::glimpse(catch_data)\n\nRows: 1,708\nColumns: 7\n$ Region  &lt;chr&gt; \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\",…\n$ Year    &lt;dbl&gt; 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 18…\n$ Chinook &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"3\", \"4\", \"5\", \"9…\n$ Sockeye &lt;dbl&gt; 5, 155, 224, 182, 251, 274, 207, 189, 253, 408, 989, 791, 708,…\n$ Coho    &lt;dbl&gt; 0, 0, 16, 11, 42, 24, 11, 1, 5, 8, 192, 161, 132, 139, 84, 107…\n$ Pink    &lt;dbl&gt; 0, 0, 0, 92, 0, 0, 8, 187, 529, 606, 996, 2218, 673, 1545, 204…\n$ Chum    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 102, 343…\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExamine the output of the glimpse() function call. Does anything seem amiss with this data set that might warrant fixing?\n\n\nAnswer:\n\nThe Chinook catch data are character class. Let’s fix it using the function mutate() before moving on.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#changing-column-content-using-mutate",
    "href": "session_10.html#changing-column-content-using-mutate",
    "title": "Cleaning and Wrangling Data",
    "section": "10.7 Changing column content using mutate()",
    "text": "10.7 Changing column content using mutate()\nWe can use the mutate() function to change a column, or to create a new column. First, let’s try to convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column.\n\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = as.numeric(Chinook))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Chinook = as.numeric(Chinook)`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(catch_clean)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886       0       5     0     0     0\n2 SSE     1887       0     155     0     0     0\n3 SSE     1888       0     224    16     0     0\n4 SSE     1889       0     182    11    92     0\n5 SSE     1890       0     251    42     0     0\n6 SSE     1891       0     274    24     0     0\n\n\nWe get a warning \"NAs introduced by coercion\" which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning data sets and it’s important to have the skills to deal with it when it comes up.\nTo investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i.\n\ni &lt;- which(is.na(catch_clean$Chinook))\ni\n\n[1] 401\n\n\nIt looks like there is only one problem row, lets have a look at it in the original data.\n\ncatch_data[i,]\n\n# A tibble: 1 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GSE     1955 I            66     0     0     1\n\n\nWell that’s odd: The value in Chinook is the letter I. It turns out that this data set is from a PDF which was automatically converted into a csv and this value of I is actually a 1.\nLet’s fix it by incorporating the if_else() function to our mutate() call, which will change the value of the Chinook column to 1 if the value is equal to I, then will use as.numeric() to turn the character representations of numbers into numeric typed values.\n\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = if_else(condition = Chinook == \"I\", \n                             true = \"1\", \n                             false = Chinook),\n           Chinook = as.numeric(Chinook))\n\n##check\ncatch_clean[i, ]\n\n# A tibble: 1 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GSE     1955       1      66     0     0     1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "href": "session_10.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "title": "Cleaning and Wrangling Data",
    "section": "10.8 Changing shape using pivot_longer() and pivot_wider()",
    "text": "10.8 Changing shape using pivot_longer() and pivot_wider()\nThe next issue is that the data are in a wide format and we want the data in a long format instead. The function pivot_longer() from the tidyr package helps us do this conversion. If you do not remember all the arguments that go into pivot_longer() you can always call the help page by typing ?pivot_longer in the console.\n\ncatch_long &lt;- catch_clean %&gt;% \n    #pivot longer all columns except Region and Year\n    pivot_longer(\n        cols = -c(Region, Year),\n        names_to = \"species\",\n        values_to = \"catch\"\n    )\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye     5\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nThe syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it.\n\nThe first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or in this case, the names of the columns you are not pivoting over.\nThe names_to argument: this is the name of the column that you are creating from the column names of the columns you are pivoting over.\nThe values_to argument: the name of the column that you are creating from the values in the columns you are pivoting over.\n\nThe opposite of pivot_longer() is the pivot_wider() function. It works in a similar declarative fashion:\n\ncatch_wide &lt;- catch_long %&gt;%\n    pivot_wider(names_from = species,\n                values_from = catch)\n\nhead(catch_wide)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886       0       5     0     0     0\n2 SSE     1887       0     155     0     0     0\n3 SSE     1888       0     224    16     0     0\n4 SSE     1889       0     182    11    92     0\n5 SSE     1890       0     251    42     0     0\n6 SSE     1891       0     274    24     0     0\n\n\nSame than we did above we can pull up the documentation of the function to remind ourselves what goes in which argument. Type ?pivot_wider in the console.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#renaming-columns-with-rename",
    "href": "session_10.html#renaming-columns-with-rename",
    "title": "Cleaning and Wrangling Data",
    "section": "10.9 Renaming columns with rename()",
    "text": "10.9 Renaming columns with rename()\nIf you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish, so let’s convert it before moving on.\nLet’s first rename the catch column to be called catch_thousands:\n\ncatch_long &lt;- catch_long %&gt;%\n    rename(catch_thousands = catch)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch_thousands\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1 SSE     1886 Chinook               0\n2 SSE     1886 Sockeye               5\n3 SSE     1886 Coho                  0\n4 SSE     1886 Pink                  0\n5 SSE     1886 Chum                  0\n6 SSE     1887 Chinook               0\n\n\n\n\n\n\n\n\nnames() versus rename()\n\n\n\nMany people use the base R function names() to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select() function from dplyr. Although these methods work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in.\nTo illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario:\nYour colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on.\nUnbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the data set, the column order is different. Your script which previously renamed the fourth column, SAL_PSU to salinity now renames the fourth column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it!\nIf you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename() function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have thrown an error in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#adding-columns-using-mutate",
    "href": "session_10.html#adding-columns-using-mutate",
    "title": "Cleaning and Wrangling Data",
    "section": "10.10 Adding columns using mutate()",
    "text": "10.10 Adding columns using mutate()\nNow let’s use mutate() again to create a new column called catch with units of fish (instead of thousands of fish).\n\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000)\n\nhead(catch_long)\n\nLet’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_clean data frame.\n\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000) %&gt;%\n    select(-catch_thousands)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nWe’re now ready to start analyzing the data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#summary-statistics-using-group_by-and-summarize",
    "href": "session_10.html#summary-statistics-using-group_by-and-summarize",
    "title": "Cleaning and Wrangling Data",
    "section": "10.11 Summary statistics using group_by() and summarize()",
    "text": "10.11 Summary statistics using group_by() and summarize()\nSuppose we are now interested in getting the average catch per region. In our initial data exploration we saw there are 18 regions, we can easily see their names again:\n\nunique(catch_original$Region)\n\n [1] \"SSE\" \"NSE\" \"YAK\" \"GSE\" \"BER\" \"COP\" \"PWS\" \"CKI\" \"BRB\" \"KSK\" \"YUK\" \"NRS\"\n[13] \"KTZ\" \"KOD\" \"CHG\" \"SOP\" \"ALU\" \"NOP\"\n\n\nThink about how we would calculate the average catch per region “by hand”. It would be something like this:\n\nWe start with our table and notice there are multiple regions in the “Regions” column.\nWe split our original table to group all observations from the same region together.\nWe calculate the average catch for each of the groups we form.\nThen we combine the values for average catch per region into a single table.\n\n\n\n\n\n\n\n\nAnalyses like this conform to what is known as the Split-Apply-Combine strategy. This strategy follows the three steps we explained above:\n\nSplit: Split the data into logical groups (e.g., region, species, etc.)\nApply: Calculate some summary statistic on each group (e.g. mean catch by year, number of individuals per species)\nCombine: Combine the statistic calculated on each group back together into a single table\n\nThe dplyr library lets us easily employ the Split-Apply-Combine strategy by using the group_by() and summarize() functions:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 ALU        40384.\n2 BER        16373.\n3 BRB      2709796.\n4 CHG       315487.\n5 CKI       683571.\n6 COP       179223.\n\n\nLet’s see how the previous code implements the Split-Apply-Combine strategy:\n\ngroup_by(Region): this is telling R to split the dataframe and create a group for each different value in the column Region. R just keeps track of the groups, it doesn’t return separate dataframes per region.\nmean(catch): here mean is the function we want to apply to the column catch in each group.\nsummarize(catch = mean(catch)) the function summarize() is used to combine the results of mean(catch) in each group into a single table. The argument mean_catch = mean(catch) indicates that the column having the results of mean(catch) will be named mean_catch.\n\nAnother common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n().\n\nn_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(n = n())\n\nhead(n_region)\n\n# A tibble: 6 × 2\n  Region     n\n  &lt;chr&gt;  &lt;int&gt;\n1 ALU      435\n2 BER      510\n3 BRB      570\n4 CHG      550\n5 CKI      525\n6 COP      470\n\n\n\n\n\n\n\n\nTry using count()\n\n\n\nIf you are finding that you are reaching for this combination of group_by(), summarize() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function!\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFind another grouping and statistic to calculate for each group.\nFind out if you can group by multiple variables.\n\n\n\nAnswer\n## for example:\ncatch_year_sp &lt;- catch_long %&gt;%\n    group_by(Year, species) %&gt;%\n    summarize(total_year = sum(catch, na.rm = T))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#filtering-rows-using-filter",
    "href": "session_10.html#filtering-rows-using-filter",
    "title": "Cleaning and Wrangling Data",
    "section": "10.12 Filtering rows using filter()",
    "text": "10.12 Filtering rows using filter()\nWe use the filter() function to filter our data.frame to rows matching some condition. It’s similar to subset() from base R.\nLet’s go back to our original data.frame and do some filter()ing:\n\nsse_catch &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\")\n\nhead(sse_catch)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFilter to just catches of over one million fish\nFilter to just Chinook from the SSE region\n\n\n\nAnswer\n## Catches over a million fish\ncatch_million &lt;- catch_long %&gt;%\n    filter(catch &gt; 1000000)\n\n## Chinook from SSE data\nchinook_see &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\",\n           species == \"Chinook\")\n\n## OR\nchinook_see &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\" & species == \"Chinook\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#sorting-your-data-using-arrange",
    "href": "session_10.html#sorting-your-data-using-arrange",
    "title": "Cleaning and Wrangling Data",
    "section": "10.13 Sorting your data using arrange()",
    "text": "10.13 Sorting your data using arrange()\nThe arrange() function is used to sort the rows of a data.frame. Two common cases to use arrange() are:\n\nTo calculate a cumulative sum (with cumsum()) so row order matters\nTo display a table (like in an .qmd document) in sorted order\n\nLet’s re-calculate mean catch by region, and then arrange() the output by mean catch:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(mean_catch)\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 BER        16373.\n2 KTZ        18836.\n3 ALU        40384.\n4 NRS        51503.\n5 KSK        67642.\n6 YUK        68646.\n\n\nThe default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(desc(mean_catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 SSE      3184661.\n2 BRB      2709796.\n3 NSE      1825021.\n4 KOD      1528350 \n5 PWS      1419237.\n6 SOP      1110942.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#splitting-a-column-using-separate-and-unite",
    "href": "session_10.html#splitting-a-column-using-separate-and-unite",
    "title": "Cleaning and Wrangling Data",
    "section": "10.14 Splitting a column using separate() and unite()",
    "text": "10.14 Splitting a column using separate() and unite()\nThe separate() function allow us to easily split a single column into numerous. Its complement, the unite() function, allows us to combine multiple columns into a single one.\nThis can come in really handy when we need to split a column into two pieces by a consistent separator (like a dash).\nLet’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information.\n\nsites_df &lt;- data.frame(site = c(\"HAW-101\",\n                                \"HAW-103\",\n                                \"OAH-320\",\n                                \"OAH-219\",\n                                \"MAU-039\"))\n\nsites_df %&gt;%\n    separate(site, c(\"island\", \"site_number\"), \"-\")\n\n  island site_number\n1    HAW         101\n2    HAW         103\n3    OAH         320\n4    OAH         219\n5    MAU         039\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSplit the city column in the data frame cities_df into city and state_code columns\n\n## create `cities_df`\ncities_df &lt;- data.frame(city = c(\"Juneau AK\",\n                                 \"Sitka AK\",\n                                 \"Anchorage AK\"))\n\n\n\nAnswer\ncolnames(cities_df)\n\ncities_clean &lt;- cities_df %&gt;%\n    separate(city, c(\"city\", \"state_code\"), \" \")\n\n\n\n\nThe unite() function does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column.\n\ndates_df &lt;- data.frame(\n    year = c(\"1930\",\n             \"1930\",\n             \"1930\"),\n    month = c(\"12\",\n              \"12\",\n              \"12\"),\n    day = c(\"14\",\n            \"15\",\n            \"16\")\n)\n\ndates_df %&gt;%\n    unite(date, year, month, day, sep = \"-\")\n\n        date\n1 1930-12-14\n2 1930-12-15\n3 1930-12-16",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html#now-all-together",
    "href": "session_10.html#now-all-together",
    "title": "Cleaning and Wrangling Data",
    "section": "10.15 Now, all together!",
    "text": "10.15 Now, all together!\nWe just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now:\n\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\nmean_region &lt;- catch_original %&gt;%\n  select(-All, -notesRegCode) %&gt;% \n  mutate(Chinook = if_else(Chinook == \"I\", \"1\", Chinook)) %&gt;% \n  mutate(Chinook = as.numeric(Chinook)) %&gt;% \n  pivot_longer(-c(Region, Year), \n               names_to = \"species\", \n               values_to = \"catch\") %&gt;%\n  mutate(catch = catch*1000) %&gt;% \n  group_by(Region) %&gt;% \n  summarize(mean_catch = mean(catch)) %&gt;% \n  arrange(desc(mean_catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 SSE      3184661.\n2 BRB      2709796.\n3 NSE      1825021.\n4 KOD      1528350 \n5 PWS      1419237.\n6 SOP      1110942.\n\n\nWe have completed our lesson on Cleaning and Wrangling data. Before we break, let’s practice our Git workflow.\n\n\n\n\n\n\nSteps\n\n\n\n\nSave the .qmd you have been working on for this lesson.\nRender the Quarto file. This is a way to test everything in your code is working.\nStage (Add) &gt; Commit &gt; Pull &gt; Push",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  }
]